{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Historical and Conceptual Lineage of Denoising to Diffusion Models\n",
        "\n",
        "## 1. Denoising predates modern machine learning by decades\n",
        "\n",
        "Denoising has been a central problem in signal processing and statistics since the mid-20th century. The core objective—remove noise while preserving structure—long predates modern deep learning.\n",
        "\n",
        "### Classical roots (1940s–1970s)\n",
        "\n",
        "**Wiener filtering (1949)**  \n",
        "Optimal *linear* denoising under Gaussian assumptions, derived from minimizing mean squared error:\n",
        "$$\n",
        "\\hat{x} = \\arg\\min_{\\tilde{x}} \\ \\mathbb{E}\\big[\\|x-\\tilde{x}\\|^2\\big]\n",
        "$$\n",
        "with solutions expressible via second-order statistics.\n",
        "\n",
        "**Kalman filtering (1960)**  \n",
        "Recursive Bayesian inference for dynamical systems:\n",
        "$$\n",
        "x_t = F x_{t-1} + \\epsilon_t,\\quad z_t = H x_t + \\eta_t,\n",
        "$$\n",
        "where inference is performed sequentially with Gaussian assumptions.\n",
        "\n",
        "**Core point**: these methods already framed denoising as statistical inference.  \n",
        "**Core limitation**: linearity and Gaussian modeling.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Nonlinear and statistical denoising (1980s–1990s)\n",
        "\n",
        "This era moved denoising beyond “variance reduction” toward “structure-aware inference,” in a way that is conceptually close to later generative denoising trajectories.\n",
        "\n",
        "### Projection pursuit and non-Gaussian structure\n",
        "\n",
        "Projection pursuit emphasized that informative signal structure often appears in non-Gaussian projections:\n",
        "$$\n",
        "y = w^\\top x,\\quad \\text{seek } w \\text{ such that } y \\text{ is maximally non-Gaussian.}\n",
        "$$\n",
        "This links “signal” to higher-order statistics, not just covariance.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Wavelet shrinkage (early 1990s)\n",
        "\n",
        "Wavelet shrinkage is a major pre-ICA milestone that introduced practical nonlinear denoising at scale.\n",
        "\n",
        "Given a transform (wavelet basis) producing coefficients $c_i$, the key heuristic insight is:\n",
        "\n",
        "- small coefficients are likely noise  \n",
        "- large coefficients are likely signal\n",
        "\n",
        "A common form is soft-thresholding:\n",
        "$$\n",
        "\\hat{c}_i = \\mathrm{sign}(c_i)\\max(|c_i|-\\lambda,0),\n",
        "$$\n",
        "followed by inverse transform reconstruction.\n",
        "\n",
        "**Conceptual leap**: denoising via nonlinear shrinkage in a coefficient space.  \n",
        "**Limitation**: the representation is fixed (not learned), and the shrinkage is typically chosen by analytic/heuristic rules.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. ICA and Sparse Code Shrinkage (mid–late 1990s)\n",
        "\n",
        "This is a direct conceptual ancestor of diffusion/score-based thinking: denoising as probabilistic inference in a learned coordinate system.\n",
        "\n",
        "### ICA representation learning\n",
        "\n",
        "ICA learns a linear transform (unmixing) that produces statistically independent, non-Gaussian coordinates:\n",
        "$$\n",
        "x = A s,\\quad s = W x,\n",
        "$$\n",
        "with independence:\n",
        "$$\n",
        "p(s) = \\prod_i p(s_i).\n",
        "$$\n",
        "\n",
        "### Sparse Code Shrinkage (SCS): denoising as inference\n",
        "\n",
        "With additive Gaussian noise:\n",
        "$$\n",
        "z = x + n,\\quad n\\sim \\mathcal{N}(0,\\sigma^2 I),\n",
        "$$\n",
        "and a sparse (super-Gaussian) prior on latent coefficients:\n",
        "$$\n",
        "p(s_i)\\propto \\exp(-\\phi(s_i)),\n",
        "$$\n",
        "SCS performs ML/MAP-style inference:\n",
        "$$\n",
        "\\hat{s}=\\arg\\max_s \\ \\log p(z\\mid s)+\\log p(s)\n",
        "\\quad \\Longleftrightarrow \\quad\n",
        "\\hat{s}=\\arg\\min_s \\left[\\frac{1}{2\\sigma^2}\\|z-As\\|^2+\\sum_i \\phi(s_i)\\right].\n",
        "$$\n",
        "\n",
        "This yields a *nonlinear*, component-wise shrinkage rule in the learned latent space:\n",
        "$$\n",
        "\\hat{s}_i = g(y_i),\\quad y = Wz,\n",
        "$$\n",
        "where $g(\\cdot)$ is derived from the assumed prior and noise model (not hand-designed thresholding).\n",
        "\n",
        "**Key point**: denoising is framed as statistical inference, not smoothing.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Energy-based and score ideas (1990s–2000s)\n",
        "\n",
        "The score function exists well before diffusion models:\n",
        "$$\n",
        "\\nabla_x \\log p(x).\n",
        "$$\n",
        "Scores appear implicitly/explicitly across:\n",
        "- energy-based modeling, where $p(x)\\propto e^{-E(x)}$ and $\\nabla_x \\log p(x) = -\\nabla_x E(x)$\n",
        "- learning rules related to non-Gaussian modeling and independence criteria\n",
        "\n",
        "**What was still missing**: an explicit framing of *generation* as a multi-step denoising trajectory driven by a controlled noise process.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. What Sohl-Dickstein introduced (2015): denoising as a generative mechanism\n",
        "\n",
        "The decisive novelty was not denoising itself, but *turning denoising into sampling* by defining:\n",
        "\n",
        "1) a forward noising process (diffusion) that maps data to noise  \n",
        "2) a learned or approximated reverse process that generates data by iterative denoising\n",
        "\n",
        "Conceptually:\n",
        "$$\n",
        "x_0 \\to x_1 \\to \\cdots \\to x_T \\quad (\\text{forward noise}),\n",
        "$$\n",
        "then sampling via:\n",
        "$$\n",
        "x_T \\to x_{T-1} \\to \\cdots \\to x_0 \\quad (\\text{reverse denoise}).\n",
        "$$\n",
        "\n",
        "This reframed generation as a time-reversed stochastic process, influenced by statistical physics and stochastic dynamics.\n",
        "\n",
        "**Not new**: Gaussian corruption and inference-based denoising.  \n",
        "**New**: generation as repeated denoising along a defined stochastic path.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. What Song unified (2019–2021): score-based modeling as iterative denoising\n",
        "\n",
        "A key consolidation was showing deep connections among:\n",
        "- score matching\n",
        "- denoising objectives\n",
        "- diffusion-like procedures\n",
        "\n",
        "The denoising view becomes explicitly score-driven, where denoising steps align with estimated scores at different noise levels.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Where Hyvärinen fits historically\n",
        "\n",
        "Hyvärinen’s contributions sit much closer to the conceptual core of diffusion models than common “novelty narratives” suggest:\n",
        "\n",
        "- non-Gaussian modeling as a signal principle  \n",
        "- sparse latent inference under explicit probabilistic assumptions  \n",
        "- shrinkage as statistically derived estimation  \n",
        "- learning a coordinate system where inference becomes simple and component-wise\n",
        "\n",
        "**What he did not do**:\n",
        "- frame denoising as a time-reversed stochastic process  \n",
        "- construct a generative sampler by repeated denoising along a noise schedule\n",
        "\n",
        "---\n",
        "\n",
        "## 9. One-sentence academic truth\n",
        "\n",
        "Diffusion models did not invent denoising; they transformed decades of statistical denoising, sparse inference, and score estimation into a generative dynamical system.\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Conceptual lineage (clean summary)\n",
        "\n",
        "$$\n",
        "\\text{Wiener / Kalman}\n",
        "\\ \\rightarrow\\\n",
        "\\text{Projection Pursuit}\n",
        "\\ \\rightarrow\\\n",
        "\\text{Wavelet Shrinkage}\n",
        "\\ \\rightarrow\\\n",
        "\\text{ICA \\& Sparse Code Shrinkage}\n",
        "\\ \\rightarrow\\\n",
        "\\text{Energy Models / Score Ideas}\n",
        "\\ \\rightarrow\\\n",
        "\\text{Denoising Autoencoders}\n",
        "\\ \\rightarrow\\\n",
        "\\text{Diffusion \\& Score-Based Models}.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Final takeaway\n",
        "\n",
        "The “novelty” of diffusion models is best stated precisely:\n",
        "\n",
        "- Denoising as inference is old.  \n",
        "- Nonlinear shrinkage and sparse priors are old.  \n",
        "- Score concepts are old.  \n",
        "- The distinctive innovation is turning denoising from a tool into a principled *sampling mechanism* via a stochastic forward process and its learned (or modeled) reverse.\n"
      ],
      "metadata": {
        "id": "7d4ZgTYZb2fC"
      }
    }
  ]
}