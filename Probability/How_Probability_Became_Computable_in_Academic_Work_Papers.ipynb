{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How Probability Became Computable  \n",
        "An intellectual journey from belief to optimization, dynamics, and generation\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Turning Probability into Optimization  \n",
        "From uncertainty to objectives\n",
        "\n",
        "### Ronald A. Fisher (1922–1925) — Maximum Likelihood Estimation\n",
        "\n",
        "**Key works**\n",
        "- *On the Mathematical Foundations of Theoretical Statistics*  \n",
        "- *Theory of Statistical Estimation*\n",
        "\n",
        "**Core Idea**\n",
        "\n",
        "Early probability theory treated distributions as objects to be calculated:\n",
        "\n",
        "$$\n",
        "p(x)\n",
        "$$\n",
        "\n",
        "But Fisher introduced a radical reframing:\n",
        "\n",
        "Do not try to compute the entire distribution.  \n",
        "Instead, find the parameters that make the observed data most probable.\n",
        "\n",
        "Formally:\n",
        "\n",
        "$$\n",
        "\\theta^\\* = \\arg\\max_\\theta \\log p(x \\mid \\theta)\n",
        "$$\n",
        "\n",
        "**What changed conceptually?**\n",
        "\n",
        "- Probability → objective function  \n",
        "- Integration → differentiation  \n",
        "- Inference → optimization  \n",
        "\n",
        "This was the first great computational rupture in statistics.\n",
        "\n",
        "**Why it matters historically**\n",
        "\n",
        "This single move transformed statistics from:\n",
        "\n",
        "- a descriptive, philosophical discipline  \n",
        "\n",
        "into:\n",
        "\n",
        "- a procedural, algorithmic science  \n",
        "\n",
        "Modern machine learning still rests on this idea.\n",
        "\n",
        "---\n",
        "\n",
        "### Edwin T. Jaynes (1957) — Maximum Entropy\n",
        "\n",
        "**Key work**\n",
        "- *Information Theory and Statistical Mechanics*\n",
        "\n",
        "**Core Idea**\n",
        "\n",
        "What if:\n",
        "\n",
        "- the distribution is unknown?  \n",
        "- the full probability is impossible to compute?  \n",
        "- only partial constraints are available?  \n",
        "\n",
        "Jaynes’ answer:\n",
        "\n",
        "Choose the distribution with maximum entropy subject to known constraints.\n",
        "\n",
        "$$\n",
        "\\max_p \\; H(p) \\quad \\text{s.t. known constraints}\n",
        "$$\n",
        "\n",
        "**Conceptual shift**\n",
        "\n",
        "- Probability → constrained optimization  \n",
        "- Ignorance → principled uncertainty  \n",
        "- Partial knowledge → computable distribution  \n",
        "\n",
        "**Philosophical importance**\n",
        "\n",
        "This established the epistemological foundation of modern Bayesian reasoning:\n",
        "\n",
        "Probability is not truth — it is rational belief under constraints.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Turning Probability into Approximation  \n",
        "From exactness to controlled error\n",
        "\n",
        "### Pierre-Simon Laplace (1774) — Laplace Approximation\n",
        "\n",
        "**Key work**\n",
        "- *Mémoire sur la probabilité des causes*\n",
        "\n",
        "**Core Idea**\n",
        "\n",
        "Exact Bayesian posteriors are often impossible to integrate.  \n",
        "Laplace proposed approximating a complex distribution locally by a Gaussian around its mode:\n",
        "\n",
        "$$\n",
        "p(x) \\approx \\mathcal{N}(\\mu, \\Sigma)\n",
        "$$\n",
        "\n",
        "**Consequences**\n",
        "\n",
        "- Intractable distribution → Gaussian  \n",
        "- Global complexity → local curvature  \n",
        "- Integration → second-order geometry  \n",
        "\n",
        "**Historical role**\n",
        "\n",
        "This approximation underlies almost all classical Bayesian statistics.\n",
        "\n",
        "---\n",
        "\n",
        "### Michael I. Jordan et al. (1999) — Variational Inference\n",
        "\n",
        "**Key work**\n",
        "- *An Introduction to Variational Methods for Graphical Models*\n",
        "\n",
        "**Core Idea**\n",
        "\n",
        "Instead of computing:\n",
        "\n",
        "$$\n",
        "p(z \\mid x)\n",
        "$$\n",
        "\n",
        "Approximate it with a tractable distribution:\n",
        "\n",
        "$$\n",
        "q(z) \\approx p(z \\mid x)\n",
        "$$\n",
        "\n",
        "By solving:\n",
        "\n",
        "$$\n",
        "\\min_q \\; \\mathrm{KL}(q \\| p)\n",
        "$$\n",
        "\n",
        "**Conceptual transformation**\n",
        "\n",
        "- Inference → optimization  \n",
        "- Integration → factorization  \n",
        "- Probability → functional approximation  \n",
        "\n",
        "**Why this is foundational**\n",
        "\n",
        "This work is the backbone of:\n",
        "\n",
        "- Variational Inference (VI)  \n",
        "- Variational Autoencoders (VAEs)  \n",
        "- Large-scale Bayesian learning  \n",
        "\n",
        "---\n",
        "\n",
        "## 3. Turning Probability into Sampling  \n",
        "From formulas to experience\n",
        "\n",
        "### Metropolis et al. (1953)\n",
        "- *Equation of State Calculations by Fast Computing Machines*\n",
        "\n",
        "### Hastings (1970)\n",
        "- *Monte Carlo Sampling Methods Using Markov Chains*\n",
        "\n",
        "**Core Idea**\n",
        "\n",
        "Do not compute the distribution — draw samples from it.\n",
        "\n",
        "- Impossible integral → sample average  \n",
        "- Probability → repeated random experiment  \n",
        "- Normalization constant → unnecessary  \n",
        "\n",
        "This shift created Markov Chain Monte Carlo (MCMC).\n",
        "\n",
        "---\n",
        "\n",
        "### Stuart Geman and Donald Geman (1984) — Gibbs Sampling\n",
        "\n",
        "**Key work**\n",
        "- *Stochastic Relaxation, Gibbs Distributions, and Bayesian Restoration*\n",
        "\n",
        "**Core Idea**\n",
        "\n",
        "Decompose a joint distribution into simple conditionals.  \n",
        "Sample each variable given the others.\n",
        "\n",
        "- Joint probability → local conditionals  \n",
        "- Foundation of graphical models  \n",
        "\n",
        "---\n",
        "\n",
        "## 4. Turning Probability into Dynamics  \n",
        "From distributions to motion\n",
        "\n",
        "### Paul Langevin (1908) — Langevin Dynamics\n",
        "\n",
        "**Key work**\n",
        "- *On the Theory of Brownian Motion*\n",
        "\n",
        "**Core Idea**\n",
        "\n",
        "Sampling can be expressed as stochastic motion:\n",
        "\n",
        "$$\n",
        "dx = \\nabla \\log p(x)\\,dt + dW_t\n",
        "$$\n",
        "\n",
        "**Conceptual leap**\n",
        "\n",
        "- Probability → force  \n",
        "- Sample → trajectory  \n",
        "- Inference → stochastic dynamics  \n",
        "\n",
        "---\n",
        "\n",
        "### Radford Neal (2011) — Hamiltonian Monte Carlo\n",
        "\n",
        "**Key work**\n",
        "- *MCMC Using Hamiltonian Dynamics*\n",
        "\n",
        "- Random walks → physical trajectories  \n",
        "- Slow mixing → energy-preserving motion  \n",
        "\n",
        "This enabled scalable Bayesian inference in high dimensions.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Turning Probability into Energy  \n",
        "From belief to physics\n",
        "\n",
        "### Geoffrey Hinton (2002)\n",
        "- *Training Products of Experts by Minimizing Contrastive Divergence*\n",
        "\n",
        "### Yann LeCun et al. (2006)\n",
        "- *A Tutorial on Energy-Based Learning*\n",
        "\n",
        "**Core Idea**\n",
        "\n",
        "Replace probability with energy:\n",
        "\n",
        "$$\n",
        "p(x) \\propto e^{-E(x)}\n",
        "$$\n",
        "\n",
        "**Meaning**\n",
        "\n",
        "- Probability → energy landscape  \n",
        "- Learning → lowering energy of data  \n",
        "- Normalization → often ignored  \n",
        "\n",
        "This is the deep root of:\n",
        "\n",
        "- Energy-Based Models (EBMs)  \n",
        "- Later diffusion and score models  \n",
        "\n",
        "---\n",
        "\n",
        "## 6. Turning Probability into Flow  \n",
        "From density to transformation\n",
        "\n",
        "### Danilo Rezende and Shakir Mohamed (2015)\n",
        "- *Variational Inference with Normalizing Flows*\n",
        "\n",
        "### Laurent Dinh et al. (2017)\n",
        "- *Density Estimation using Real NVP*\n",
        "\n",
        "- Complex distribution → invertible transformations  \n",
        "- Exact likelihood via Jacobian  \n",
        "\n",
        "---\n",
        "\n",
        "## 7. Turning Probability into a Vector Field  \n",
        "From densities to directions\n",
        "\n",
        "### Aapo Hyvärinen (2005) — Score Matching\n",
        "- *Estimation of Non-Normalized Statistical Models by Score Matching*\n",
        "\n",
        "### Yang Song et al. (2019–2021)\n",
        "- *Score-Based Generative Modeling through SDEs*\n",
        "\n",
        "**Core Idea**\n",
        "\n",
        "Do not learn:\n",
        "\n",
        "$$\n",
        "p(x)\n",
        "$$\n",
        "\n",
        "Learn instead:\n",
        "\n",
        "$$\n",
        "\\nabla_x \\log p(x)\n",
        "$$\n",
        "\n",
        "**Why this matters**\n",
        "\n",
        "- Distribution → direction  \n",
        "- Generation → solving reverse differential equations  \n",
        "- Normalization → irrelevant  \n",
        "\n",
        "This is the current apex of diffusion models.\n",
        "\n",
        "---\n",
        "\n",
        "## Philosophical Synthesis\n",
        "\n",
        "All these breakthroughs say the same thing, in different languages:\n",
        "\n",
        "If probability is intractable, change the question.\n",
        "\n",
        "Do not ask:\n",
        "\n",
        "- What is the probability?\n",
        "\n",
        "Ask instead:\n",
        "\n",
        "- What can be optimized?  \n",
        "- What can be approximated?  \n",
        "- What can be sampled?  \n",
        "- What can be simulated dynamically?  \n",
        "\n",
        "---\n",
        "\n",
        "## Final Insight\n",
        "\n",
        "Probability did not become simpler.  \n",
        "Our questions became computational.\n",
        "\n",
        "This is how probability evolved:\n",
        "\n",
        "- from a closed theoretical science  \n",
        "\n",
        "into:\n",
        "\n",
        "- the engine of modern generative intelligence\n"
      ],
      "metadata": {
        "id": "XSFXltmpNlVJ"
      }
    }
  ]
}