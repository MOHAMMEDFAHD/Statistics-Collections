{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Turning Probability into Dynamics or Optimization  \n",
        "When distributions become trajectories or objectives\n",
        "\n",
        "---\n",
        "\n",
        "## 1. The Core Motivation\n",
        "\n",
        "Exact probability is a static object:\n",
        "\n",
        "$$\n",
        "p(x), \\qquad \\int p(x)\\,dx = 1\n",
        "$$\n",
        "\n",
        "But static objects are hard:\n",
        "\n",
        "- normalization constants are intractable  \n",
        "- marginalization is exponential  \n",
        "- likelihoods are unavailable  \n",
        "\n",
        "So we ask a radical question:\n",
        "\n",
        "Can we replace **“what is the probability?”** with **“how does it change?”** or **“what should be optimized?”**\n",
        "\n",
        "This leads to two transformations:\n",
        "\n",
        "- Probability → Optimization  \n",
        "- Probability → Dynamics  \n",
        "\n",
        "---\n",
        "\n",
        "## 2. Probability → Optimization  \n",
        "Inference becomes finding extrema\n",
        "\n",
        "### 2.1 Log-Probability as an Objective\n",
        "\n",
        "Instead of manipulating  \n",
        "$$\n",
        "p(x)\n",
        "$$\n",
        "work with:\n",
        "\n",
        "$$\n",
        "\\log p(x)\n",
        "$$\n",
        "\n",
        "Why?\n",
        "\n",
        "- products → sums  \n",
        "- numerical stability  \n",
        "- gradients exist  \n",
        "\n",
        "This already turns probability into geometry.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.2 Maximum Likelihood Estimation (MLE)\n",
        "\n",
        "$$\n",
        "\\theta^\\* = \\arg\\max_\\theta \\log p(x \\mid \\theta)\n",
        "$$\n",
        "\n",
        "Key shift:\n",
        "\n",
        "- distribution → single best parameter  \n",
        "- integration → differentiation  \n",
        "\n",
        "Interpretation:\n",
        "\n",
        "Probability becomes a **loss function**.\n",
        "\n",
        "This is the foundation of:\n",
        "\n",
        "- regression  \n",
        "- classification  \n",
        "- neural networks  \n",
        "\n",
        "---\n",
        "\n",
        "### 2.3 Maximum A Posteriori (MAP)\n",
        "\n",
        "$$\n",
        "\\theta^\\* = \\arg\\max_\\theta \\log p(x \\mid \\theta) + \\log p(\\theta)\n",
        "$$\n",
        "\n",
        "Adds:\n",
        "\n",
        "- prior knowledge  \n",
        "- regularization  \n",
        "\n",
        "MAP collapses Bayesian inference into deterministic optimization.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.4 Energy-Based Reformulation\n",
        "\n",
        "Define:\n",
        "\n",
        "$$\n",
        "E(x) = -\\log p(x)\n",
        "\\quad \\Rightarrow \\quad\n",
        "p(x) = \\frac{e^{-E(x)}}{Z}\n",
        "$$\n",
        "\n",
        "Now:\n",
        "\n",
        "- high probability = low energy  \n",
        "- inference = energy minimization  \n",
        "\n",
        "This reframes probability as physics.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.5 Variational Inference (Optimization over Distributions)\n",
        "\n",
        "Instead of optimizing parameters, optimize functions:\n",
        "\n",
        "$$\n",
        "q^\\* = \\arg\\min_q \\mathrm{KL}(q \\| p)\n",
        "$$\n",
        "\n",
        "Equivalent to maximizing:\n",
        "\n",
        "$$\n",
        "\\mathrm{ELBO}\n",
        "=\n",
        "\\mathbb{E}_q[\\log p]\n",
        "-\n",
        "\\mathbb{E}_q[\\log q]\n",
        "$$\n",
        "\n",
        "Here:\n",
        "\n",
        "- probability → functional objective  \n",
        "- inference → calculus of variations  \n",
        "\n",
        "This is probability as optimization in function space.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Probability → Dynamics  \n",
        "Inference becomes motion\n",
        "\n",
        "Instead of asking:\n",
        "\n",
        "What is  \n",
        "$$\n",
        "p(x)\n",
        "$$\n",
        "\n",
        "We ask:\n",
        "\n",
        "How does a system move so that its equilibrium is  \n",
        "$$\n",
        "p(x)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Gradient Flow Interpretation\n",
        "\n",
        "### 4.1 Probability as a Landscape\n",
        "\n",
        "Consider:\n",
        "\n",
        "$$\n",
        "\\log p(x)\n",
        "$$\n",
        "\n",
        "This defines a surface.  \n",
        "Gradients point toward higher probability.\n",
        "\n",
        "---\n",
        "\n",
        "### 4.2 Langevin Dynamics\n",
        "\n",
        "$$\n",
        "dx_t\n",
        "=\n",
        "\\nabla \\log p(x_t)\\,dt\n",
        "+\n",
        "\\sqrt{2}\\,dW_t\n",
        "$$\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "- gradient ascent → move toward high probability  \n",
        "- noise → explore uncertainty  \n",
        "\n",
        "This is:\n",
        "\n",
        "- sampling  \n",
        "- optimization  \n",
        "- diffusion  \n",
        "\n",
        "all at once.\n",
        "\n",
        "Langevin dynamics is probability turned into motion.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Stochastic Differential Equations (SDEs)\n",
        "\n",
        "General form:\n",
        "\n",
        "$$\n",
        "dx = f(x,t)\\,dt + g(t)\\,dW\n",
        "$$\n",
        "\n",
        "Here:\n",
        "\n",
        "- probability is not computed  \n",
        "- probability is generated  \n",
        "\n",
        "The distribution of  \n",
        "$$\n",
        "x_t\n",
        "$$\n",
        "*is* the probability.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.1 Fokker–Planck Equation\n",
        "\n",
        "Instead of tracking particles, track density:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial p}{\\partial t}\n",
        "=\n",
        "-\\nabla \\cdot (f p)\n",
        "+\n",
        "\\frac{1}{2}\\nabla^2(g^2 p)\n",
        "$$\n",
        "\n",
        "This converts:\n",
        "\n",
        "- stochastic dynamics → deterministic PDE  \n",
        "- randomness → density flow  \n",
        "\n",
        "---\n",
        "\n",
        "## 6. Reverse-Time Dynamics\n",
        "\n",
        "A profound insight:\n",
        "\n",
        "If you know how probability diffuses forward, you can reverse it.\n",
        "\n",
        "Reverse SDE:\n",
        "\n",
        "$$\n",
        "dx\n",
        "=\n",
        "\\big[\n",
        "f(x,t)\n",
        "-\n",
        "g^2(t)\\nabla \\log p_t(x)\n",
        "\\big]\\,dt\n",
        "+\n",
        "g(t)\\,d\\bar{W}\n",
        "$$\n",
        "\n",
        "This requires only:\n",
        "\n",
        "$$\n",
        "\\nabla \\log p_t(x)\n",
        "$$\n",
        "\n",
        "Not  \n",
        "$$\n",
        "p(x)\n",
        "$$\n",
        "itself.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Score-Based Models  \n",
        "Probability via vector fields\n",
        "\n",
        "Instead of learning  \n",
        "$$\n",
        "p(x)\n",
        "$$\n",
        "learn:\n",
        "\n",
        "$$\n",
        "s(x,t) = \\nabla_x \\log p_t(x)\n",
        "$$\n",
        "\n",
        "Why this matters:\n",
        "\n",
        "- no normalization  \n",
        "- local information only  \n",
        "- dimension-agnostic  \n",
        "\n",
        "Generation equals integrating a differential equation.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Diffusion Models  \n",
        "Probability as reversible destruction\n",
        "\n",
        "Forward process:\n",
        "\n",
        "$$\n",
        "x_t\n",
        "=\n",
        "\\alpha_t x_0\n",
        "+\n",
        "\\sqrt{1-\\alpha_t}\\,\\epsilon\n",
        "$$\n",
        "\n",
        "Reverse process:\n",
        "\n",
        "- remove noise gradually  \n",
        "- guided by learned score  \n",
        "\n",
        "This replaces density estimation with trajectory synthesis.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Optimal Transport and Probability Flow\n",
        "\n",
        "### 9.1 Probability as Mass Movement\n",
        "\n",
        "Instead of random motion, move probability mass optimally:\n",
        "\n",
        "$$\n",
        "\\int \\|x - T(x)\\|^2 \\, d\\mu(x)\n",
        "$$\n",
        "\n",
        "This yields:\n",
        "\n",
        "- deterministic flows  \n",
        "- invertible mappings  \n",
        "\n",
        "Used in:\n",
        "\n",
        "- normalizing flows  \n",
        "- probability flow ODEs  \n",
        "\n",
        "---\n",
        "\n",
        "## 10. Probability Flow ODEs\n",
        "\n",
        "Replace SDE:\n",
        "\n",
        "$$\n",
        "dx = f(x,t)\\,dt + g(t)\\,dW\n",
        "$$\n",
        "\n",
        "With deterministic ODE:\n",
        "\n",
        "$$\n",
        "dx\n",
        "=\n",
        "\\big[\n",
        "f(x,t)\n",
        "-\n",
        "\\frac{1}{2}g^2(t)\\nabla \\log p_t(x)\n",
        "\\big]\\,dt\n",
        "$$\n",
        "\n",
        "This gives:\n",
        "\n",
        "- exact likelihood  \n",
        "- deterministic generation  \n",
        "\n",
        "---\n",
        "\n",
        "## 11. Optimization–Dynamics Duality\n",
        "\n",
        "A unifying principle:\n",
        "\n",
        "| Optimization | Dynamics |\n",
        "|-------------|----------|\n",
        "| Gradient descent | Deterministic flow |\n",
        "| Stochastic gradient descent | Langevin dynamics |\n",
        "| Loss landscape | Energy landscape |\n",
        "| Convergence | Stationary distribution |\n",
        "\n",
        "Optimization is frozen dynamics.  \n",
        "Dynamics is noisy optimization.\n",
        "\n",
        "---\n",
        "\n",
        "## 12. Why This Works (Deep Reason)\n",
        "\n",
        "Probability is hard because:\n",
        "\n",
        "- it is global  \n",
        "- it requires integration  \n",
        "- it needs normalization  \n",
        "\n",
        "Dynamics and optimization:\n",
        "\n",
        "- are local  \n",
        "- use gradients  \n",
        "- avoid normalization  \n",
        "\n",
        "They turn global structure into local motion.\n",
        "\n",
        "---\n",
        "\n",
        "## 13. Failure Modes\n",
        "\n",
        "This transformation fails when:\n",
        "\n",
        "- gradients are inaccurate  \n",
        "- landscapes are multimodal  \n",
        "- dynamics mix slowly  \n",
        "- optimization collapses diversity  \n",
        "\n",
        "This explains:\n",
        "\n",
        "- mode collapse  \n",
        "- overconfident models  \n",
        "- poor uncertainty estimates  \n",
        "\n",
        "---\n",
        "\n",
        "## 14. The Meta Insight\n",
        "\n",
        "Probability does not need to be represented to be used.  \n",
        "It only needs to guide motion or descent.\n",
        "\n",
        "This is why modern generative models:\n",
        "\n",
        "- don’t compute likelihoods  \n",
        "- don’t normalize densities  \n",
        "- don’t store distributions  \n",
        "\n",
        "They **move through probability**.\n",
        "\n",
        "---\n",
        "\n",
        "## 15. Final Synthesis\n",
        "\n",
        "Turning probability into dynamics or optimization replaces  \n",
        "**“belief as a function”** with **“belief as behavior.”**\n",
        "\n",
        "Instead of asking:\n",
        "\n",
        "- What is likely?\n",
        "\n",
        "We ask:\n",
        "\n",
        "- Where does the system move?  \n",
        "- What does it converge to?\n",
        "\n",
        "This is the deepest computational reframing of probability ever achieved.\n"
      ],
      "metadata": {
        "id": "ce6X1SBb18rT"
      }
    }
  ]
}