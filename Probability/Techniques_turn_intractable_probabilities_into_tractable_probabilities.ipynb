{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# I. Analytical Simplification Techniques\n",
        "\n",
        "Make the math solvable on paper.\n",
        "\n",
        "## 1. Independence Assumptions\n",
        "\n",
        "Factor joint distributions:\n",
        "\n",
        "$$\n",
        "p(x_1,\\dots,x_n)=\\prod_i p(x_i)\n",
        "$$\n",
        "\n",
        "Core of Naive Bayes, classical statistics.\n",
        "\n",
        "## 2. Conditional Independence\n",
        "\n",
        "Use graphical structure to reduce dimensionality.\n",
        "\n",
        "Foundation of Bayesian Networks, HMMs.\n",
        "\n",
        "## 3. Sufficient Statistics\n",
        "\n",
        "Compress data without losing information about parameters.\n",
        "\n",
        "Exponential family magic.\n",
        "\n",
        "## 4. Conjugate Priors\n",
        "\n",
        "Posterior has same functional form as prior.\n",
        "\n",
        "Enables closed-form Bayesian updates.\n",
        "\n",
        "## 5. Change of Variables\n",
        "\n",
        "Jacobian tricks to simplify distributions.\n",
        "\n",
        "Basis of normalizing flows (modern revival).\n",
        "\n",
        "## 6. Moment Closure\n",
        "\n",
        "Approximate infinite hierarchies by truncating moments.\n",
        "\n",
        "Used in physics, population models.\n",
        "\n",
        "---\n",
        "\n",
        "# II. Approximation via Optimization\n",
        "\n",
        "Replace probability with optimization.\n",
        "\n",
        "## 7. Maximum Likelihood Estimation (MLE)\n",
        "\n",
        "Replace integrals with argmax problems.\n",
        "\n",
        "## 8. Maximum A Posteriori (MAP)\n",
        "\n",
        "Bayesian inference → optimization.\n",
        "\n",
        "## 9. Laplace Approximation\n",
        "\n",
        "Approximate posterior by a Gaussian around the mode.\n",
        "\n",
        "One of the oldest asymptotic tools.\n",
        "\n",
        "## 10. Saddle-Point Approximation\n",
        "\n",
        "High-precision asymptotics for integrals.\n",
        "\n",
        "Widely used in statistical physics.\n",
        "\n",
        "---\n",
        "\n",
        "# III. Sampling-Based Techniques (Monte Carlo World)\n",
        "\n",
        "Replace integrals with averages.\n",
        "\n",
        "## 11. Monte Carlo Integration\n",
        "\n",
        "Law of Large Numbers turns expectation into arithmetic.\n",
        "\n",
        "## 12. Importance Sampling\n",
        "\n",
        "Reweight samples from easier distributions.\n",
        "\n",
        "## 13. Rejection Sampling\n",
        "\n",
        "Sample from complex distributions using envelopes.\n",
        "\n",
        "## 14. Markov Chain Monte Carlo (MCMC)\n",
        "\n",
        "Sample without knowing normalization constant.\n",
        "\n",
        "Metropolis–Hastings\n",
        "\n",
        "Gibbs Sampling\n",
        "\n",
        "Hamiltonian Monte Carlo (HMC)\n",
        "\n",
        "Langevin Dynamics\n",
        "\n",
        "## 15. Sequential Monte Carlo (Particle Filters)\n",
        "\n",
        "Time-evolving distributions.\n",
        "\n",
        "---\n",
        "\n",
        "# IV. Variational & Functional Approximations\n",
        "\n",
        "Turn inference into function fitting.\n",
        "\n",
        "## 16. Variational Inference (VI)\n",
        "\n",
        "Minimize KL divergence instead of computing integrals.\n",
        "\n",
        "## 17. Mean-Field Approximation\n",
        "\n",
        "Break dependencies into independent factors.\n",
        "\n",
        "## 18. Expectation Propagation (EP)\n",
        "\n",
        "Local moment matching instead of global KL.\n",
        "\n",
        "## 19. Free Energy Minimization\n",
        "\n",
        "Physics → statistics bridge.\n",
        "\n",
        "Basis of ELBO.\n",
        "\n",
        "---\n",
        "\n",
        "# V. Transformational Representations\n",
        "\n",
        "Change the space so probability becomes easy.\n",
        "\n",
        "## 20. Fourier / Characteristic Functions\n",
        "\n",
        "Convolution → multiplication.\n",
        "\n",
        "## 21. Generating Functions\n",
        "\n",
        "Encode distributions into algebraic objects.\n",
        "\n",
        "## 22. Laplace Transforms\n",
        "\n",
        "Differential equations → algebraic equations.\n",
        "\n",
        "## 23. Probability Generating Functions\n",
        "\n",
        "Discrete distributions tractability.\n",
        "\n",
        "---\n",
        "\n",
        "# VI. Structural Decomposition\n",
        "\n",
        "Exploit structure instead of brute force.\n",
        "\n",
        "## 24. Graphical Models\n",
        "\n",
        "Factorization via graphs.\n",
        "\n",
        "Bayesian Networks\n",
        "\n",
        "Markov Random Fields\n",
        "\n",
        "## 25. Message Passing Algorithms\n",
        "\n",
        "Belief Propagation\n",
        "\n",
        "Sum-Product / Max-Product\n",
        "\n",
        "## 26. Dynamic Programming\n",
        "\n",
        "HMM forward-backward\n",
        "\n",
        "Viterbi algorithm\n",
        "\n",
        "---\n",
        "\n",
        "# VII. Asymptotic & Limit Theorems\n",
        "\n",
        "Let infinity do the work.\n",
        "\n",
        "## 27. Law of Large Numbers\n",
        "\n",
        "Random → deterministic.\n",
        "\n",
        "## 28. Central Limit Theorem\n",
        "\n",
        "Everything → Gaussian.\n",
        "\n",
        "## 29. Large Deviations Theory\n",
        "\n",
        "Exponentially small probabilities become analyzable.\n",
        "\n",
        "---\n",
        "\n",
        "# VIII. Continuous-Time & Differential Methods\n",
        "\n",
        "Probability as dynamics.\n",
        "\n",
        "## 30. Stochastic Differential Equations (SDEs)\n",
        "\n",
        "Replace distributions with stochastic flows.\n",
        "\n",
        "## 31. Fokker–Planck Equations\n",
        "\n",
        "Track density evolution deterministically.\n",
        "\n",
        "## 32. Score Matching\n",
        "\n",
        "Learn gradients instead of densities.\n",
        "\n",
        "## 33. Diffusion & Reverse-Time Processes\n",
        "\n",
        "Probability generation via dynamics.\n",
        "\n",
        "---\n",
        "\n",
        "# IX. Discretization & Relaxation\n",
        "\n",
        "Trade exactness for computability.\n",
        "\n",
        "## 34. Discretization of Continuous Spaces\n",
        "\n",
        "Grids, bins, quantization.\n",
        "\n",
        "## 35. Relaxation of Constraints\n",
        "\n",
        "Replace hard constraints with penalties.\n",
        "\n",
        "## 36. Surrogate Objectives\n",
        "\n",
        "Lower bounds, upper bounds.\n",
        "\n",
        "---\n",
        "\n",
        "# X. Symmetry & Invariance Exploitation\n",
        "\n",
        "Reduce degrees of freedom.\n",
        "\n",
        "## 37. Exchangeability\n",
        "\n",
        "Leads to de Finetti representation.\n",
        "\n",
        "## 38. Stationarity & Ergodicity\n",
        "\n",
        "Time averages = ensemble averages.\n",
        "\n",
        "---\n",
        "\n",
        "# XI. Learning-Based Tractability\n",
        "\n",
        "Let models learn the probability for you.\n",
        "\n",
        "## 39. Density Estimation Models\n",
        "\n",
        "Autoregressive models\n",
        "\n",
        "Normalizing Flows\n",
        "\n",
        "Energy-Based Models\n",
        "\n",
        "## 40. Implicit Models\n",
        "\n",
        "GANs (sampling without likelihoods).\n",
        "\n",
        "## 41. Score-Based Models\n",
        "\n",
        "Avoid density, learn vector fields.\n",
        "\n",
        "---\n",
        "\n",
        "# XII. Decision-Theoretic Collapse\n",
        "\n",
        "Probability becomes action.\n",
        "\n",
        "## 42. Bayes Risk Minimization\n",
        "\n",
        "Integrals replaced by expected loss.\n",
        "\n",
        "## 43. Proper Scoring Rules\n",
        "\n",
        "Turn distribution learning into optimization.\n"
      ],
      "metadata": {
        "id": "MojjS_WazpQw"
      }
    }
  ]
}