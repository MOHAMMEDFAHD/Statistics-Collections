{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Probability & Statistics — Structured Foundations (Clean Terminology Map)\n",
        "\n",
        "This is a coherent roadmap of the concepts you listed, organized so each layer builds the next.  \n",
        "For each layer: (i) core objects, (ii) what is fixed vs variable, (iii) the governing equations.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Foundations of Probability (What randomness means)\n",
        "\n",
        "### Core objects\n",
        "- **Random variable** (discrete vs continuous): a measurable function\n",
        "  $$\n",
        "  X:\\Omega \\to \\mathbb{R}\n",
        "  $$\n",
        "- **Sample space**: $\\Omega$ (all possible outcomes)\n",
        "- **Event**: $A \\subseteq \\Omega$\n",
        "- **Probability measure**: $P:\\mathcal{F}\\to[0,1]$ on a $\\sigma$-algebra $\\mathcal{F}$\n",
        "\n",
        "### Kolmogorov axioms\n",
        "For $A,B\\in\\mathcal{F}$:\n",
        "1. Non-negativity:  \n",
        "   $$\n",
        "   P(A)\\ge 0\n",
        "   $$\n",
        "2. Normalization:  \n",
        "   $$\n",
        "   P(\\Omega)=1\n",
        "   $$\n",
        "3. Countable additivity (disjoint $A_i$):  \n",
        "   $$\n",
        "   P\\Big(\\bigcup_{i=1}^\\infty A_i\\Big)=\\sum_{i=1}^\\infty P(A_i)\n",
        "   $$\n",
        "\n",
        "### Distribution functions\n",
        "- **PMF** (discrete):\n",
        "  $$\n",
        "  p(x)=P(X=x), \\quad \\sum_x p(x)=1\n",
        "  $$\n",
        "- **PDF** (continuous):\n",
        "  $$\n",
        "  f(x)\\ge 0,\\quad \\int_{-\\infty}^{\\infty} f(x)\\,dx=1,\\quad\n",
        "  P(a\\le X\\le b)=\\int_a^b f(x)\\,dx\n",
        "  $$\n",
        "- **CDF** (both):\n",
        "  $$\n",
        "  F(x)=P(X\\le x)\n",
        "  $$\n",
        "- **Support**:\n",
        "  - discrete: $\\{x: p(x)>0\\}$\n",
        "  - continuous: region where $f(x)>0$ (up to measure-zero)\n",
        "\n",
        "**Goal**: probability assigns consistent numbers to events, and distributions are how that assignment appears on the real line via $X$.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Probability Distributions (Generative models)\n",
        "\n",
        "A **distribution family** is a set of possible worlds indexed by parameters:\n",
        "$$\n",
        "\\{P_\\theta : \\theta \\in \\Theta\\}\n",
        "\\quad\\text{or}\\quad\n",
        "\\{p(x\\mid \\theta)\\}\n",
        "$$\n",
        "\n",
        "### Core distributions (typical parameterizations)\n",
        "- **Bernoulli**: $X\\in\\{0,1\\}$  \n",
        "  $$\n",
        "  P(X=1)=p,\\quad P(X=0)=1-p\n",
        "  $$\n",
        "- **Binomial**: $X\\in\\{0,\\dots,n\\}$  \n",
        "  $$\n",
        "  P(X=k)=\\binom{n}{k}p^k(1-p)^{n-k}\n",
        "  $$\n",
        "- **Multinomial**: counts $x_1,\\dots,x_K$ with $\\sum x_k=n$  \n",
        "  $$\n",
        "  P(\\mathbf{x})=\\frac{n!}{\\prod_{k=1}^K x_k!}\\prod_{k=1}^K p_k^{x_k}\n",
        "  $$\n",
        "- **Poisson**: $X\\in\\{0,1,2,\\dots\\}$  \n",
        "  $$\n",
        "  P(X=k)=\\frac{\\lambda^k e^{-\\lambda}}{k!}\n",
        "  $$\n",
        "- **Uniform** on $[a,b]$  \n",
        "  $$\n",
        "  f(x)=\\frac{1}{b-a},\\quad a\\le x\\le b\n",
        "  $$\n",
        "- **Normal (Gaussian)**  \n",
        "  $$\n",
        "  f(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\!\\Big(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\Big)\n",
        "  $$\n",
        "- **Exponential**  \n",
        "  $$\n",
        "  f(x)=\\lambda e^{-\\lambda x},\\quad x\\ge 0\n",
        "  $$\n",
        "- **Gamma** (shape $\\alpha$, rate $\\beta$)  \n",
        "  $$\n",
        "  f(x)=\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\beta x},\\quad x\\ge 0\n",
        "  $$\n",
        "- **Beta** (on $[0,1]$)  \n",
        "  $$\n",
        "  f(x)=\\frac{1}{B(\\alpha,\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}\n",
        "  $$\n",
        "\n",
        "### Structural concepts\n",
        "- **Parametric distribution**: $p(x\\mid\\theta)$ with finite-dimensional $\\theta$\n",
        "- **Parameters**:\n",
        "  - location (e.g., $\\mu$), scale (e.g., $\\sigma$), shape (e.g., $\\alpha$)\n",
        "- **Moments**:\n",
        "  $$\n",
        "  \\mathbb{E}[X],\\quad \\mathrm{Var}(X)=\\mathbb{E}[X^2]-\\mathbb{E}[X]^2\n",
        "  $$\n",
        "  Higher: skewness, kurtosis\n",
        "- **MGF** (if exists):\n",
        "  $$\n",
        "  M_X(t)=\\mathbb{E}[e^{tX}]\n",
        "  $$\n",
        "- **Characteristic function** (always exists):\n",
        "  $$\n",
        "  \\varphi_X(t)=\\mathbb{E}[e^{itX}]\n",
        "  $$\n",
        "\n",
        "**Goal**: distributions are families of data-generating worlds, not just formulas.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Conditioning & Dependence (Information flow)\n",
        "\n",
        "### Core definitions\n",
        "- **Joint distribution**: $p(x,y)$ or $f(x,y)$\n",
        "- **Marginal distribution**:\n",
        "  $$\n",
        "  p(x)=\\sum_y p(x,y)\n",
        "  \\quad\\text{or}\\quad\n",
        "  f(x)=\\int f(x,y)\\,dy\n",
        "  $$\n",
        "- **Conditional probability**:\n",
        "  $$\n",
        "  P(A\\mid B)=\\frac{P(A\\cap B)}{P(B)},\\quad P(B)>0\n",
        "  $$\n",
        "- **Conditional density/mass**:\n",
        "  $$\n",
        "  p(x\\mid y)=\\frac{p(x,y)}{p(y)}\n",
        "  $$\n",
        "\n",
        "### Laws\n",
        "- **Chain rule**:\n",
        "  $$\n",
        "  p(x_1,\\dots,x_n)=\\prod_{i=1}^n p(x_i\\mid x_1,\\dots,x_{i-1})\n",
        "  $$\n",
        "- **Independence**:\n",
        "  $$\n",
        "  X\\perp Y \\iff p(x,y)=p(x)p(y)\n",
        "  $$\n",
        "- **Conditional independence**:\n",
        "  $$\n",
        "  X\\perp Y \\mid Z \\iff p(x,y\\mid z)=p(x\\mid z)p(y\\mid z)\n",
        "  $$\n",
        "- **Bayes’ theorem**:\n",
        "  $$\n",
        "  p(\\theta\\mid x)=\\frac{p(x\\mid \\theta)p(\\theta)}{p(x)}\n",
        "  $$\n",
        "- **Law of total probability**:\n",
        "  $$\n",
        "  p(x)=\\sum_\\theta p(x\\mid \\theta)p(\\theta)\n",
        "  \\quad\\text{or}\\quad\n",
        "  p(x)=\\int p(x\\mid \\theta)p(\\theta)\\,d\\theta\n",
        "  $$\n",
        "\n",
        "**Goal**: conditioning is how information updates probabilities.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Likelihood Theory (Inference mindset)\n",
        "\n",
        "Given data $x$ and model $p(x\\mid\\theta)$:\n",
        "\n",
        "- **Likelihood function**:\n",
        "  $$\n",
        "  L(\\theta\\mid x)=p(x\\mid\\theta)\n",
        "  $$\n",
        "- **Log-likelihood**:\n",
        "  $$\n",
        "  \\ell(\\theta\\mid x)=\\log L(\\theta\\mid x)=\\log p(x\\mid\\theta)\n",
        "  $$\n",
        "- **Likelihood surface**: $\\theta \\mapsto L(\\theta\\mid x)$\n",
        "- **Relative likelihood**:\n",
        "  $$\n",
        "  \\frac{L(\\theta_1\\mid x)}{L(\\theta_2\\mid x)}\n",
        "  $$\n",
        "- **Likelihood ratio**:\n",
        "  $$\n",
        "  \\Lambda(x)=\\frac{\\sup_{\\theta\\in\\Theta_0} L(\\theta\\mid x)}{\\sup_{\\theta\\in\\Theta} L(\\theta\\mid x)}\n",
        "  $$\n",
        "- **Identifiability**:\n",
        "  $$\n",
        "  p(x\\mid\\theta_1)=p(x\\mid\\theta_2)\\ \\forall x \\ \\Rightarrow\\ \\theta_1=\\theta_2\n",
        "  $$\n",
        "- **Sufficient statistic** (factorization idea):\n",
        "  $$\n",
        "  p(x\\mid\\theta)=g(T(x),\\theta)\\,h(x)\n",
        "  $$\n",
        "\n",
        "**Critical insight**: the formula is the same as probability, but the *question* is reversed: data fixed, parameters variable.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Parameter Estimation (From data → model)\n",
        "\n",
        "### Maximum Likelihood Estimation (MLE)\n",
        "$$\n",
        "\\hat\\theta_{\\text{MLE}}=\\arg\\max_{\\theta} L(\\theta\\mid x)\n",
        "=\\arg\\max_{\\theta}\\ \\ell(\\theta\\mid x)\n",
        "$$\n",
        "\n",
        "### Score function (gradient of log-likelihood)\n",
        "$$\n",
        "s(\\theta)=\\nabla_\\theta \\ell(\\theta\\mid x)\n",
        "$$\n",
        "\n",
        "### Fisher Information\n",
        "Observed (data-dependent):\n",
        "$$\n",
        "\\mathcal{I}_{\\text{obs}}(\\theta)=-\\nabla_\\theta^2 \\ell(\\theta\\mid x)\n",
        "$$\n",
        "Expected:\n",
        "$$\n",
        "\\mathcal{I}(\\theta)=\\mathbb{E}\\big[s(\\theta)s(\\theta)^\\top\\big]\n",
        "= -\\mathbb{E}\\big[\\nabla_\\theta^2 \\ell(\\theta\\mid X)\\big]\n",
        "$$\n",
        "\n",
        "### Cramér–Rao lower bound (unbiased estimators)\n",
        "$$\n",
        "\\mathrm{Var}(\\hat\\theta)\\ \\ge\\ \\mathcal{I}(\\theta)^{-1}\n",
        "$$\n",
        "\n",
        "### Bias, variance, consistency, asymptotics\n",
        "- Bias:\n",
        "  $$\n",
        "  \\mathrm{Bias}(\\hat\\theta)=\\mathbb{E}[\\hat\\theta]-\\theta\n",
        "  $$\n",
        "- Consistency:\n",
        "  $$\n",
        "  \\hat\\theta_n \\xrightarrow[]{P} \\theta\n",
        "  $$\n",
        "- Asymptotic normality (typical MLE result):\n",
        "  $$\n",
        "  \\sqrt{n}\\,(\\hat\\theta_n-\\theta)\\ \\Rightarrow\\ \\mathcal{N}\\big(0,\\mathcal{I}(\\theta)^{-1}\\big)\n",
        "  $$\n",
        "\n",
        "**Goal**: understand why MLE works (geometry/curvature via Fisher information), not only how to compute it.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Bayesian Perspective (Probability over models)\n",
        "\n",
        "Bayesian inference treats parameters as random variables.\n",
        "\n",
        "- **Prior**: $p(\\theta)$  \n",
        "- **Likelihood**: $p(x\\mid\\theta)$  \n",
        "- **Posterior**:\n",
        "  $$\n",
        "  p(\\theta\\mid x)=\\frac{p(x\\mid\\theta)p(\\theta)}{p(x)}\n",
        "  $$\n",
        "- **Evidence / marginal likelihood**:\n",
        "  $$\n",
        "  p(x)=\\int p(x\\mid\\theta)p(\\theta)\\,d\\theta\n",
        "  $$\n",
        "- **Posterior predictive**:\n",
        "  $$\n",
        "  p(x_{\\text{new}}\\mid x)=\\int p(x_{\\text{new}}\\mid\\theta)\\,p(\\theta\\mid x)\\,d\\theta\n",
        "  $$\n",
        "- **Bayes factor** (model comparison):\n",
        "  $$\n",
        "  BF_{12}=\\frac{p(x\\mid M_1)}{p(x\\mid M_2)}\n",
        "  $$\n",
        "- **Conjugate priors**: priors that keep posterior in same family\n",
        "- **MAP estimation**:\n",
        "  $$\n",
        "  \\hat\\theta_{\\text{MAP}}=\\arg\\max_{\\theta}\\ p(\\theta\\mid x)\n",
        "  =\\arg\\max_{\\theta}\\ \\big[\\log p(x\\mid\\theta)+\\log p(\\theta)\\big]\n",
        "  $$\n",
        "\n",
        "**Conceptual leap**: parameters become random variables; inference becomes probability calculus.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Information-Theoretic View (Geometry of probability)\n",
        "\n",
        "- **Entropy**:\n",
        "  $$\n",
        "  H(P)=-\\sum_x p(x)\\log p(x)\n",
        "  \\quad\\text{or}\\quad\n",
        "  h(P)=-\\int f(x)\\log f(x)\\,dx\n",
        "  $$\n",
        "- **Cross-entropy**:\n",
        "  $$\n",
        "  H(P,Q)=-\\sum_x p(x)\\log q(x)\n",
        "  $$\n",
        "- **KL divergence**:\n",
        "  $$\n",
        "  D_{\\mathrm{KL}}(P\\|Q)=\\sum_x p(x)\\log\\frac{p(x)}{q(x)}\n",
        "  \\quad\\text{or}\\quad\n",
        "  \\int f(x)\\log\\frac{f(x)}{g(x)}\\,dx\n",
        "  $$\n",
        "- **Mutual information**:\n",
        "  $$\n",
        "  I(X;Y)=D_{\\mathrm{KL}}(p(x,y)\\ \\|\\ p(x)p(y))\n",
        "  $$\n",
        "- **Maximum entropy principle**: choose the distribution with maximal entropy under constraints\n",
        "- **Log-loss**: negative log-likelihood per sample (typical learning objective)\n",
        "\n",
        "**Unification**: likelihood, learning, compression, and generalization often reduce to minimizing cross-entropy / KL.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Monte Carlo & Sampling (Probability as a generator)\n",
        "\n",
        "- **Monte Carlo estimation** (for $\\mu=\\mathbb{E}[f(X)]$):\n",
        "  $$\n",
        "  \\hat\\mu=\\frac{1}{N}\\sum_{i=1}^N f(X_i),\\quad X_i\\sim p\n",
        "  $$\n",
        "- **Law of Large Numbers**:\n",
        "  $$\n",
        "  \\hat\\mu \\xrightarrow[]{a.s.} \\mu\n",
        "  $$\n",
        "- **Central Limit Theorem**:\n",
        "  $$\n",
        "  \\sqrt{N}(\\hat\\mu-\\mu)\\ \\Rightarrow\\ \\mathcal{N}(0,\\sigma^2)\n",
        "  $$\n",
        "- **Importance sampling**:\n",
        "  $$\n",
        "  \\mathbb{E}_p[f(X)]=\\mathbb{E}_q\\!\\left[f(X)\\frac{p(X)}{q(X)}\\right]\n",
        "  \\approx \\frac{1}{N}\\sum_{i=1}^N f(X_i)\\frac{p(X_i)}{q(X_i)}\n",
        "  $$\n",
        "- **Rejection sampling**: accept/reject using envelope constant $M$\n",
        "- **MCMC**: build a Markov chain with stationary distribution $p$\n",
        "  - **Metropolis–Hastings**: accept with\n",
        "    $$\n",
        "    \\alpha=\\min\\!\\left(1,\\frac{p(x')q(x\\mid x')}{p(x)q(x'\\mid x)}\\right)\n",
        "    $$\n",
        "  - **Gibbs sampling**: sample from conditionals $p(x_i\\mid x_{-i})$\n",
        "- **Langevin dynamics** (score-driven sampling idea):\n",
        "  $$\n",
        "  x_{t+1}=x_t+\\frac{\\epsilon}{2}\\nabla_x \\log p(x_t)+\\sqrt{\\epsilon}\\,z_t,\\quad z_t\\sim \\mathcal{N}(0,I)\n",
        "  $$\n",
        "\n",
        "**Goal**: probability becomes computation: turning distributions into samples.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Statistical Modeling View (Putting it all together)\n",
        "\n",
        "- **Generative vs discriminative**\n",
        "  $$\n",
        "  \\text{generative: } p(x,y)\\ \\text{ or } p(x\\mid\\theta)\n",
        "  \\qquad\n",
        "  \\text{discriminative: } p(y\\mid x)\n",
        "  $$\n",
        "- **Latent variable models**: introduce unobserved $z$:\n",
        "  $$\n",
        "  p(x)=\\int p(x,z)\\,dz\n",
        "  $$\n",
        "- **Mixture models**:\n",
        "  $$\n",
        "  p(x)=\\sum_{k=1}^K \\pi_k\\,p(x\\mid \\theta_k)\n",
        "  $$\n",
        "- **Expectation–Maximization (EM)** (maximize likelihood with latent variables):\n",
        "  - E-step: compute $p(z\\mid x,\\theta^{old})$\n",
        "  - M-step: maximize expected complete log-likelihood\n",
        "- **Exponential family**:\n",
        "  $$\n",
        "  p(x\\mid\\eta)=h(x)\\exp\\big(\\eta^\\top T(x)-A(\\eta)\\big)\n",
        "  $$\n",
        "- **Identifiability and overfitting**: distinct parameters vs too-flexible models\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Conceptual Meta-Ideas (Expert-level understanding)\n",
        "\n",
        "- **Forward vs inverse problems**:\n",
        "  $$\n",
        "  \\theta \\to x \\quad\\text{(forward)}\n",
        "  \\qquad\\text{vs}\\qquad\n",
        "  x \\to \\theta \\quad\\text{(inverse)}\n",
        "  $$\n",
        "- **Data-generating process**: the (unknown) mechanism producing observations\n",
        "- **Model misspecification**: true process not in $\\{p(\\cdot\\mid\\theta)\\}$\n",
        "- **Epistemic vs aleatoric uncertainty**:\n",
        "  - epistemic: uncertainty about the model/parameters\n",
        "  - aleatoric: inherent randomness in outcomes\n",
        "- **Frequentist vs Bayesian**: parameters fixed vs random\n",
        "- **Inference vs prediction**: explain $\\theta$ vs forecast new $x$\n",
        "- **Probability as belief vs frequency**: interpretation layer that changes what “probability” *means* philosophically\n",
        "\n",
        "---\n",
        "\n",
        "## Minimal “Must-Master” Core (Sharp Path)\n",
        "\n",
        "If reduced to a non-negotiable core:\n",
        "\n",
        "1. Random variables and distributions  \n",
        "2. Conditional probability and Bayes’ theorem  \n",
        "3. Likelihood and log-likelihood  \n",
        "4. Maximum Likelihood Estimation  \n",
        "5. Entropy and KL divergence  \n",
        "6. Monte Carlo sampling  \n",
        "\n",
        "Master these deeply, and most of the remaining structure becomes a consequence of them.\n"
      ],
      "metadata": {
        "id": "yzhreJB0I_W6"
      }
    }
  ]
}