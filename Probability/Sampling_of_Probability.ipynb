{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sampling of Probability  \n",
        "How randomness becomes a computational tool\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Why Sampling Exists\n",
        "\n",
        "Probability becomes intractable when:\n",
        "\n",
        "- integrals cannot be solved,\n",
        "- normalization constants are unknown,\n",
        "- distributions are only implicitly defined,\n",
        "- dimensionality explodes.\n",
        "\n",
        "Sampling answers a radical question:\n",
        "\n",
        "What if we never compute probability at all — and only draw from it?\n",
        "\n",
        "Instead of:\n",
        "\n",
        "$$\n",
        "\\int f(x)\\,p(x)\\,dx\n",
        "$$\n",
        "\n",
        "We use:\n",
        "\n",
        "$$\n",
        "\\frac{1}{N}\\sum_{i=1}^{N} f(x_i),\n",
        "\\qquad x_i \\sim p\n",
        "$$\n",
        "\n",
        "Sampling replaces analysis with experience.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. The Core Idea\n",
        "\n",
        "Sampling rests on one fundamental principle:\n",
        "\n",
        "**Law of Large Numbers**\n",
        "\n",
        "$$\n",
        "\\frac{1}{N}\\sum_{i=1}^{N} f(x_i)\n",
        "\\;\\xrightarrow[N \\to \\infty]{}\\;\n",
        "\\mathbb{E}_p[f(x)]\n",
        "$$\n",
        "\n",
        "This transforms:\n",
        "\n",
        "- expectation → average  \n",
        "- uncertainty → variance  \n",
        "- integration → arithmetic  \n",
        "\n",
        "Sampling makes probability empirical.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. What Sampling Actually Approximates\n",
        "\n",
        "Sampling does **not** approximate the distribution directly.  \n",
        "It approximates **functionals** of the distribution:\n",
        "\n",
        "- means  \n",
        "- variances  \n",
        "- probabilities of events  \n",
        "- expectations  \n",
        "- marginals  \n",
        "\n",
        "Formally:\n",
        "\n",
        "$$\n",
        "p(x) \\quad \\text{is never approximated}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbb{E}_p[f(x)] \\quad \\text{is}\n",
        "$$\n",
        "\n",
        "This distinction is critical.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Direct Sampling (The Ideal Case)\n",
        "\n",
        "If we can sample exactly:\n",
        "\n",
        "- inversion sampling  \n",
        "- ancestral sampling in graphical models  \n",
        "- known distributions (Gaussian, Gamma, etc.)\n",
        "\n",
        "Then:\n",
        "\n",
        "- estimates are unbiased  \n",
        "- convergence is guaranteed  \n",
        "- error is purely stochastic  \n",
        "\n",
        "But exact sampling is rare in complex models.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Monte Carlo Sampling\n",
        "\n",
        "### 5.1 Monte Carlo Estimation\n",
        "\n",
        "Given samples:\n",
        "\n",
        "$$\n",
        "x_1,\\ldots,x_N \\sim p(x)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbb{E}_p[f(x)]\n",
        "\\approx\n",
        "\\frac{1}{N}\\sum_i f(x_i)\n",
        "$$\n",
        "\n",
        "Properties:\n",
        "\n",
        "- error proportional to $\\frac{1}{\\sqrt{N}}$  \n",
        "- dimension independent  \n",
        "- slow but universal  \n",
        "\n",
        "Monte Carlo trades determinism for universality.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Importance Sampling\n",
        "\n",
        "Sampling from the wrong distribution on purpose.\n",
        "\n",
        "Rewrite expectation:\n",
        "\n",
        "$$\n",
        "\\mathbb{E}_p[f(x)]\n",
        "=\n",
        "\\mathbb{E}_q\\!\\left[\n",
        "f(x)\\frac{p(x)}{q(x)}\n",
        "\\right]\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $q(x)$ is easy to sample  \n",
        "- weights correct the bias  \n",
        "\n",
        "**Strengths**\n",
        "\n",
        "- simple  \n",
        "- parallel  \n",
        "- unbiased  \n",
        "\n",
        "**Failure Modes**\n",
        "\n",
        "- weight collapse  \n",
        "- infinite variance  \n",
        "\n",
        "Importance sampling fails when probability mass is misaligned.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Rejection Sampling\n",
        "\n",
        "Accept or reject reality.\n",
        "\n",
        "Procedure:\n",
        "\n",
        "- sample from easy $q(x)$  \n",
        "- accept with probability\n",
        "\n",
        "$$\n",
        "\\frac{p(x)}{M q(x)}\n",
        "$$\n",
        "\n",
        "Exact but inefficient in high dimensions.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Markov Chain Monte Carlo (MCMC)\n",
        "\n",
        "When direct sampling is impossible, we sample indirectly.\n",
        "\n",
        "Key idea:\n",
        "\n",
        "Construct a Markov chain whose stationary distribution is:\n",
        "\n",
        "$$\n",
        "p(x)\n",
        "$$\n",
        "\n",
        "Then:\n",
        "\n",
        "$$\n",
        "x_1 \\to x_2 \\to \\cdots \\to x_T\n",
        "$$\n",
        "\n",
        "After burn-in:\n",
        "\n",
        "$$\n",
        "x_t \\sim p(x)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 8.1 Metropolis–Hastings\n",
        "\n",
        "Accept proposal $x'$ with probability:\n",
        "\n",
        "$$\n",
        "\\alpha\n",
        "=\n",
        "\\min\\!\\left(\n",
        "1,\n",
        "\\frac{p(x')\\,q(x\\mid x')}\n",
        "     {p(x)\\,q(x'\\mid x)}\n",
        "\\right)\n",
        "$$\n",
        "\n",
        "Requires:\n",
        "\n",
        "- only density ratios  \n",
        "- no normalization constant  \n",
        "\n",
        "---\n",
        "\n",
        "### 8.2 Gibbs Sampling\n",
        "\n",
        "Sample conditionals:\n",
        "\n",
        "$$\n",
        "x_i \\sim p(x_i \\mid x_{-i})\n",
        "$$\n",
        "\n",
        "Works when conditionals are easy.\n",
        "\n",
        "---\n",
        "\n",
        "### 8.3 Hamiltonian Monte Carlo (HMC)\n",
        "\n",
        "- introduce momentum variables  \n",
        "- sample using physics  \n",
        "- avoid random walk behavior  \n",
        "\n",
        "Used in:\n",
        "\n",
        "- Bayesian neural networks  \n",
        "- high-dimensional inference  \n",
        "\n",
        "---\n",
        "\n",
        "### 8.4 Langevin Dynamics\n",
        "\n",
        "$$\n",
        "x_{t+1}\n",
        "=\n",
        "x_t\n",
        "+\n",
        "\\epsilon \\nabla \\log p(x_t)\n",
        "+\n",
        "\\sqrt{2\\epsilon}\\,\\xi_t\n",
        "$$\n",
        "\n",
        "Combines:\n",
        "\n",
        "- gradient ascent  \n",
        "- stochastic noise  \n",
        "\n",
        "This is the bridge between sampling and optimization.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Sequential Monte Carlo (Particle Methods)\n",
        "\n",
        "Used for time-evolving distributions.\n",
        "\n",
        "Represent distribution as:\n",
        "\n",
        "$$\n",
        "\\{x_t^{(i)}, w_t^{(i)}\\}_{i=1}^{N}\n",
        "$$\n",
        "\n",
        "Includes:\n",
        "\n",
        "- resampling  \n",
        "- propagation  \n",
        "- weighting  \n",
        "\n",
        "Used in:\n",
        "\n",
        "- tracking  \n",
        "- robotics  \n",
        "- filtering  \n",
        "- online inference  \n",
        "\n",
        "---\n",
        "\n",
        "## 10. Sampling via Dynamics\n",
        "\n",
        "Probability as motion.\n",
        "\n",
        "Instead of sampling from $p(x)$, simulate:\n",
        "\n",
        "- stochastic differential equations  \n",
        "- diffusion processes  \n",
        "- reverse-time dynamics  \n",
        "\n",
        "These methods:\n",
        "\n",
        "- avoid likelihoods  \n",
        "- generate samples via trajectories  \n",
        "- scale to very complex distributions  \n",
        "\n",
        "This is the foundation of diffusion models.\n",
        "\n",
        "---\n",
        "\n",
        "## 11. Approximate Sampling\n",
        "\n",
        "Sometimes exactness is sacrificed:\n",
        "\n",
        "- short chains  \n",
        "- discretized dynamics  \n",
        "- biased proposals  \n",
        "\n",
        "This introduces:\n",
        "\n",
        "- bias  \n",
        "- faster convergence  \n",
        "- practical feasibility  \n",
        "\n",
        "Sampling becomes engineering, not mathematics.\n",
        "\n",
        "---\n",
        "\n",
        "## 12. Sampling vs Approximation vs Factorization\n",
        "\n",
        "| Aspect | Factorization | Approximation | Sampling |\n",
        "|------|--------------|---------------|----------|\n",
        "| What changes | Structure | Representation | Computation |\n",
        "| Error | Structural | Bias | Variance |\n",
        "| Guarantees | Exact | Approximate | Asymptotic |\n",
        "\n",
        "Sampling is:\n",
        "\n",
        "- unbiased  \n",
        "- slow  \n",
        "- universal  \n",
        "\n",
        "---\n",
        "\n",
        "## 13. What Sampling Really Does\n",
        "\n",
        "Sampling:\n",
        "\n",
        "- turns integrals into averages  \n",
        "- avoids normalization  \n",
        "- preserves full distribution shape  \n",
        "- introduces randomness instead of bias  \n",
        "\n",
        "Sampling is the only method that does not assume what probability looks like.\n",
        "\n",
        "---\n",
        "\n",
        "## 14. Deep Philosophical Insight\n",
        "\n",
        "Sampling reframes probability as:\n",
        "\n",
        "*A distribution is not something you compute — it is something you experience.*\n",
        "\n",
        "This mirrors:\n",
        "\n",
        "- physical measurement  \n",
        "- empirical science  \n",
        "- stochastic reality  \n",
        "\n",
        "---\n",
        "\n",
        "## 15. Failure Modes of Sampling\n",
        "\n",
        "Sampling fails when:\n",
        "\n",
        "- mixing is slow  \n",
        "- dimensionality is extreme  \n",
        "- energy landscapes are multimodal  \n",
        "- variance explodes  \n",
        "\n",
        "This explains:\n",
        "\n",
        "- why GANs exist  \n",
        "- why variational inference exists  \n",
        "- why diffusion exists  \n",
        "\n",
        "---\n",
        "\n",
        "## 16. Final Synthesis\n",
        "\n",
        "Sampling replaces mathematical certainty with statistical certainty.\n",
        "\n",
        "It works because:\n",
        "\n",
        "- averages converge,  \n",
        "- noise cancels,  \n",
        "- structure emerges from randomness.  \n",
        "\n",
        "Sampling is not a hack — it is how probability becomes computable.\n"
      ],
      "metadata": {
        "id": "F0eQLPxn1fU7"
      }
    }
  ]
}