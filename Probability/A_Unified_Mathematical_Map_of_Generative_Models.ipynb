{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A Unified Mathematical Map of Generative Models\n",
        "\n",
        "This document rewrites and **normalizes** your content into a clean, consistent mathematical narrative.  \n",
        "Nothing new is added conceptually; the goal is **clarity, correctness, and structural alignment** across all families of generative models.\n",
        "\n",
        "---\n",
        "\n",
        "## 0. Shared Probabilistic Foundation\n",
        "\n",
        "### Data distribution and model\n",
        "$$\n",
        "x \\sim p_{\\text{data}}(x), \\qquad p_\\theta(x) \\;\\text{(model)}\n",
        "$$\n",
        "\n",
        "### Maximum Likelihood Estimation (MLE)\n",
        "$$\n",
        "\\theta^\\* = \\arg\\max_\\theta \\; \\mathbb{E}_{x \\sim p_{\\text{data}}}\n",
        "\\big[ \\log p_\\theta(x) \\big]\n",
        "$$\n",
        "\n",
        "### Cross-entropy and KL relations\n",
        "$$\n",
        "\\mathbb{E}_{p_{\\text{data}}}\\big[-\\log p_\\theta(x)\\big]\n",
        "= H(p_{\\text{data}}, p_\\theta)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathrm{KL}(p_{\\text{data}} \\,\\|\\, p_\\theta)\n",
        "= \\mathbb{E}_{p_{\\text{data}}}\n",
        "\\left[\n",
        "\\log \\frac{p_{\\text{data}}(x)}{p_\\theta(x)}\n",
        "\\right]\n",
        "$$\n",
        "\n",
        "Minimizing cross-entropy is equivalent to minimizing forward KL (up to a constant).\n",
        "\n",
        "### Bayes rule\n",
        "$$\n",
        "p_\\theta(z \\mid x)\n",
        "=\n",
        "\\frac{p_\\theta(x \\mid z)\\,p(z)}{p_\\theta(x)},\n",
        "\\qquad\n",
        "p_\\theta(x)=\\int p_\\theta(x \\mid z)p(z)\\,dz\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Autoregressive Generative Models\n",
        "\n",
        "### Chain rule factorization\n",
        "$$\n",
        "p_\\theta(x)\n",
        "=\n",
        "\\prod_{t=1}^{T} p_\\theta(x_t \\mid x_{<t})\n",
        "$$\n",
        "\n",
        "### Log-likelihood\n",
        "$$\n",
        "\\log p_\\theta(x)\n",
        "=\n",
        "\\sum_{t=1}^{T} \\log p_\\theta(x_t \\mid x_{<t})\n",
        "$$\n",
        "\n",
        "### Discrete softmax parameterization\n",
        "$$\n",
        "p_\\theta(x_t=v \\mid x_{<t})\n",
        "=\n",
        "\\frac{\\exp(\\ell_{\\theta,v}(x_{<t}))}\n",
        "{\\sum_{v'} \\exp(\\ell_{\\theta,v'}(x_{<t}))}\n",
        "$$\n",
        "\n",
        "Examples: NADE, PixelRNN, PixelCNN, Transformers as language models.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Latent-Variable Models (General)\n",
        "\n",
        "### Marginal likelihood\n",
        "$$\n",
        "p_\\theta(x)\n",
        "=\n",
        "\\int p_\\theta(x \\mid z)p(z)\\,dz\n",
        "$$\n",
        "\n",
        "### Variational inference  \n",
        "Introduce $q_\\phi(z \\mid x)$:\n",
        "$$\n",
        "\\log p_\\theta(x)\n",
        "=\n",
        "\\underbrace{\n",
        "\\mathbb{E}_{q_\\phi(z \\mid x)}\n",
        "[\\log p_\\theta(x \\mid z)]\n",
        "-\n",
        "\\mathrm{KL}(q_\\phi(z \\mid x)\\,\\|\\,p(z))\n",
        "}_{\\mathcal{L}_{\\text{ELBO}}(x)}\n",
        "+\n",
        "\\mathrm{KL}(q_\\phi(z \\mid x)\\,\\|\\,p_\\theta(z \\mid x))\n",
        "$$\n",
        "\n",
        "Hence:\n",
        "$$\n",
        "\\log p_\\theta(x) \\ge \\mathcal{L}_{\\text{ELBO}}(x)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Variational Autoencoders (VAE)\n",
        "\n",
        "### Standard ELBO\n",
        "$$\n",
        "\\max_{\\theta,\\phi}\n",
        "\\;\n",
        "\\mathbb{E}_{x \\sim p_{\\text{data}}}\n",
        "\\left[\n",
        "\\mathbb{E}_{q_\\phi(z \\mid x)}[\\log p_\\theta(x \\mid z)]\n",
        "-\n",
        "\\mathrm{KL}(q_\\phi(z \\mid x)\\,\\|\\,p(z))\n",
        "\\right]\n",
        "$$\n",
        "\n",
        "### Typical choices\n",
        "$$\n",
        "p(z)=\\mathcal{N}(0,I)\n",
        "$$\n",
        "\n",
        "$$\n",
        "q_\\phi(z \\mid x)=\n",
        "\\mathcal{N}(\\mu_\\phi(x), \\mathrm{diag}(\\sigma_\\phi^2(x)))\n",
        "$$\n",
        "\n",
        "### Reparameterization trick\n",
        "$$\n",
        "z=\\mu_\\phi(x)+\\sigma_\\phi(x)\\odot\\epsilon,\n",
        "\\qquad\n",
        "\\epsilon\\sim\\mathcal{N}(0,I)\n",
        "$$\n",
        "\n",
        "### $\\beta$-VAE\n",
        "$$\n",
        "\\mathcal{L}_{\\beta\\text{-VAE}}\n",
        "=\n",
        "\\mathbb{E}_{q_\\phi(z \\mid x)}[\\log p_\\theta(x \\mid z)]\n",
        "-\n",
        "\\beta\\,\\mathrm{KL}(q_\\phi(z \\mid x)\\,\\|\\,p(z))\n",
        "$$\n",
        "\n",
        "### IWAE\n",
        "$$\n",
        "\\log p_\\theta(x)\n",
        "\\ge\n",
        "\\mathbb{E}_{z_{1:K}\\sim q_\\phi}\n",
        "\\left[\n",
        "\\log\\frac{1}{K}\n",
        "\\sum_{k=1}^K\n",
        "\\frac{p_\\theta(x,z_k)}{q_\\phi(z_k\\mid x)}\n",
        "\\right]\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Flow-Based Generative Models\n",
        "\n",
        "### Invertible transform\n",
        "$$\n",
        "x=f_\\theta(z),\n",
        "\\qquad\n",
        "z=f_\\theta^{-1}(x)\n",
        "$$\n",
        "\n",
        "### Change of variables\n",
        "$$\n",
        "p_\\theta(x)\n",
        "=\n",
        "p(z)\\left|\n",
        "\\det\\frac{\\partial z}{\\partial x}\n",
        "\\right|\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\log p_\\theta(x)\n",
        "=\n",
        "\\log p(z)\n",
        "+\n",
        "\\log\\left|\n",
        "\\det\\frac{\\partial f_\\theta^{-1}(x)}{\\partial x}\n",
        "\\right|\n",
        "$$\n",
        "\n",
        "### Composition of flows\n",
        "$$\n",
        "z_0=x,\\quad\n",
        "z_k=f_k(z_{k-1}),\\quad\n",
        "z_K=z\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\log p(x)\n",
        "=\n",
        "\\log p(z_K)\n",
        "+\n",
        "\\sum_{k=1}^K\n",
        "\\log\n",
        "\\left|\n",
        "\\det\\frac{\\partial f_k}{\\partial z_{k-1}}\n",
        "\\right|\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Energy-Based Models (EBMs)\n",
        "\n",
        "### Energy parameterization\n",
        "$$\n",
        "p_\\theta(x)=\n",
        "\\frac{\\exp(-E_\\theta(x))}{Z(\\theta)},\n",
        "\\qquad\n",
        "Z(\\theta)=\\int \\exp(-E_\\theta(x))\\,dx\n",
        "$$\n",
        "\n",
        "### Log-likelihood\n",
        "$$\n",
        "\\log p_\\theta(x)\n",
        "=\n",
        "- E_\\theta(x) - \\log Z(\\theta)\n",
        "$$\n",
        "\n",
        "### Score\n",
        "$$\n",
        "\\nabla_x \\log p_\\theta(x)\n",
        "=\n",
        "- \\nabla_x E_\\theta(x)\n",
        "$$\n",
        "\n",
        "### Langevin sampling\n",
        "$$\n",
        "x_{k+1}\n",
        "=\n",
        "x_k\n",
        "+\n",
        "\\frac{\\eta}{2}\\nabla_x \\log p_\\theta(x_k)\n",
        "+\n",
        "\\sqrt{\\eta}\\,\\epsilon_k,\n",
        "\\qquad\n",
        "\\epsilon_k\\sim\\mathcal{N}(0,I)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 6. RBMs, Boltzmann Machines, Deep Belief Nets\n",
        "\n",
        "### RBM energy\n",
        "$$\n",
        "E_\\theta(v,h)\n",
        "=\n",
        "- b^\\top v\n",
        "- c^\\top h\n",
        "- v^\\top W h\n",
        "$$\n",
        "\n",
        "### Joint and marginal\n",
        "$$\n",
        "p_\\theta(v,h)=\\frac{e^{-E_\\theta(v,h)}}{Z(\\theta)},\n",
        "\\qquad\n",
        "p_\\theta(v)=\\sum_h p_\\theta(v,h)\n",
        "$$\n",
        "\n",
        "### Conditional independence\n",
        "$$\n",
        "p(h_j=1\\mid v)\n",
        "=\n",
        "\\sigma\\!\\left(c_j+\\sum_i W_{ij}v_i\\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "p(v_i=1\\mid h)\n",
        "=\n",
        "\\sigma\\!\\left(b_i+\\sum_j W_{ij}h_j\\right)\n",
        "$$\n",
        "\n",
        "### Contrastive divergence gradient\n",
        "$$\n",
        "\\nabla_\\theta \\log p_\\theta(v)\n",
        "=\n",
        "\\mathbb{E}_{p(h\\mid v)}[\\nabla_\\theta(-E_\\theta(v,h))]\n",
        "-\n",
        "\\mathbb{E}_{p(v,h)}[\\nabla_\\theta(-E_\\theta(v,h))]\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 7. GANs as Divergence Minimization\n",
        "\n",
        "### Minimax objective\n",
        "$$\n",
        "\\min_G \\max_D\n",
        "\\;\n",
        "\\mathbb{E}_{x\\sim p_{\\text{data}}}[\\log D(x)]\n",
        "+\n",
        "\\mathbb{E}_{z\\sim p(z)}[\\log(1-D(G(z)))]\n",
        "$$\n",
        "\n",
        "### Optimal discriminator\n",
        "$$\n",
        "D^\\*(x)=\n",
        "\\frac{p_{\\text{data}}(x)}\n",
        "{p_{\\text{data}}(x)+p_g(x)}\n",
        "$$\n",
        "\n",
        "At $D^\\*$, the generator minimizes Jensen–Shannon divergence.\n",
        "\n",
        "### WGAN (Wasserstein-1)\n",
        "$$\n",
        "W_1(p,q)\n",
        "=\n",
        "\\sup_{\\|f\\|_L\\le 1}\n",
        "\\mathbb{E}_{p}[f(x)]-\\mathbb{E}_{q}[f(x)]\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Diffusion Models (DDPM)\n",
        "\n",
        "### Forward noising\n",
        "$$\n",
        "q(x_t\\mid x_{t-1})\n",
        "=\n",
        "\\mathcal{N}(\\sqrt{1-\\beta_t}\\,x_{t-1},\\beta_t I)\n",
        "$$\n",
        "\n",
        "Define:\n",
        "$$\n",
        "\\alpha_t=1-\\beta_t,\n",
        "\\qquad\n",
        "\\bar{\\alpha}_t=\\prod_{s=1}^t \\alpha_s\n",
        "$$\n",
        "\n",
        "$$\n",
        "q(x_t\\mid x_0)\n",
        "=\n",
        "\\mathcal{N}(\\sqrt{\\bar{\\alpha}_t}x_0,(1-\\bar{\\alpha}_t)I)\n",
        "$$\n",
        "\n",
        "### Reverse model\n",
        "$$\n",
        "p_\\theta(x_{t-1}\\mid x_t)\n",
        "=\n",
        "\\mathcal{N}(\\mu_\\theta(x_t,t),\\Sigma_\\theta(x_t,t))\n",
        "$$\n",
        "\n",
        "### Noise prediction\n",
        "$$\n",
        "\\epsilon_\\theta(x_t,t)\\approx\\epsilon\n",
        "$$\n",
        "\n",
        "### Training loss\n",
        "$$\n",
        "\\min_\\theta\n",
        "\\;\n",
        "\\mathbb{E}_{t,x_0,\\epsilon}\n",
        "\\big[\\|\\epsilon-\\epsilon_\\theta(x_t,t)\\|_2^2\\big]\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Score Matching\n",
        "\n",
        "### Score definition\n",
        "$$\n",
        "s_\\theta(x)=\\nabla_x \\log p_\\theta(x)\n",
        "$$\n",
        "\n",
        "### Hyvärinen score matching\n",
        "$$\n",
        "J(\\theta)\n",
        "=\n",
        "\\mathbb{E}_{p_{\\text{data}}}\n",
        "\\left[\n",
        "\\frac{1}{2}\\|s_\\theta(x)\\|^2\n",
        "+\n",
        "\\nabla\\cdot s_\\theta(x)\n",
        "\\right]\n",
        "$$\n",
        "\n",
        "### Denoising score matching (Gaussian noise)\n",
        "$$\n",
        "\\min_\\theta\n",
        "\\;\n",
        "\\mathbb{E}\n",
        "\\big[\n",
        "\\|s_\\theta(\\tilde{x})-\\nabla_{\\tilde{x}}\\log q_\\sigma(\\tilde{x}\\mid x)\\|^2\n",
        "\\big]\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\nabla_{\\tilde{x}}\\log q_\\sigma(\\tilde{x}\\mid x)\n",
        "=\n",
        "\\frac{x-\\tilde{x}}{\\sigma^2}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Diffusion as SDE\n",
        "\n",
        "### Forward SDE\n",
        "$$\n",
        "dx=f(x,t)\\,dt+g(t)\\,dW_t\n",
        "$$\n",
        "\n",
        "### Reverse-time SDE\n",
        "$$\n",
        "dx=\n",
        "\\big[f(x,t)-g(t)^2\\nabla_x\\log p_t(x)\\big]dt\n",
        "+\n",
        "g(t)\\,d\\bar{W}_t\n",
        "$$\n",
        "\n",
        "### Probability flow ODE\n",
        "$$\n",
        "\\frac{dx}{dt}\n",
        "=\n",
        "f(x,t)-\\frac{1}{2}g(t)^2\\nabla_x\\log p_t(x)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 11. Discrete Latent Code Models (VQ-VAE)\n",
        "\n",
        "### Quantization\n",
        "$$\n",
        "z_e=E_\\phi(x),\n",
        "\\qquad\n",
        "z_q=e_k,\\quad\n",
        "k=\\arg\\min_j\\|z_e-e_j\\|\n",
        "$$\n",
        "\n",
        "### VQ-VAE loss\n",
        "$$\n",
        "\\mathcal{L}\n",
        "=\n",
        "\\|x-D_\\theta(z_q)\\|^2\n",
        "+\n",
        "\\| \\mathrm{sg}[z_e]-e\\|^2\n",
        "+\n",
        "\\beta\\|z_e-\\mathrm{sg}[e]\\|^2\n",
        "$$\n",
        "\n",
        "### Prior over codes\n",
        "$$\n",
        "p(k)=\\prod_t p(k_t\\mid k_{<t})\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 12. Mixture Models\n",
        "\n",
        "### Mixture density\n",
        "$$\n",
        "p(x)=\\sum_{k=1}^K \\pi_k p(x\\mid k),\n",
        "\\qquad\n",
        "\\sum_k \\pi_k=1\n",
        "$$\n",
        "\n",
        "### Gaussian mixture\n",
        "$$\n",
        "p(x)=\\sum_{k=1}^K \\pi_k \\mathcal{N}(x\\mid\\mu_k,\\Sigma_k)\n",
        "$$\n",
        "\n",
        "### EM responsibilities\n",
        "$$\n",
        "\\gamma_{nk}\n",
        "=\n",
        "\\frac{\\pi_k \\mathcal{N}(x_n\\mid\\mu_k,\\Sigma_k)}\n",
        "{\\sum_j \\pi_j \\mathcal{N}(x_n\\mid\\mu_j,\\Sigma_j)}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 13. Divergences Used in Generative Training\n",
        "\n",
        "### Forward KL (mode-covering)\n",
        "$$\n",
        "\\min_\\theta \\mathrm{KL}(p_{\\text{data}}\\|p_\\theta)\n",
        "$$\n",
        "\n",
        "### Reverse KL (mode-seeking)\n",
        "$$\n",
        "\\min_\\theta \\mathrm{KL}(p_\\theta\\|p_{\\text{data}})\n",
        "$$\n",
        "\n",
        "### f-divergence\n",
        "$$\n",
        "D_f(p\\|q)\n",
        "=\n",
        "\\int q(x)\\,f\\!\\left(\\frac{p(x)}{q(x)}\\right)\\,dx\n",
        "$$\n",
        "\n",
        "### Jensen–Shannon\n",
        "$$\n",
        "\\mathrm{JS}(p\\|q)\n",
        "=\n",
        "\\frac{1}{2}\\mathrm{KL}\\!\\left(p\\Big\\|\\frac{p+q}{2}\\right)\n",
        "+\n",
        "\\frac{1}{2}\\mathrm{KL}\\!\\left(q\\Big\\|\\frac{p+q}{2}\\right)\n",
        "$$\n",
        "\n",
        "### Wasserstein-1\n",
        "$$\n",
        "W_1(p,q)\n",
        "=\n",
        "\\inf_{\\gamma\\in\\Pi(p,q)}\n",
        "\\mathbb{E}_{(x,y)\\sim\\gamma}\\big[\\|x-y\\|\\big]\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Final Meta-View\n",
        "\n",
        "Every generative model above is a **different operationalization** of:\n",
        "\n",
        "$$\n",
        "\\text{Simple distribution}\n",
        "\\;\\xrightarrow{\\text{learned transformation}}\\;\n",
        "\\text{Data distribution}\n",
        "$$\n",
        "\n",
        "What changes is **how probability mass is moved**:  \n",
        "likelihoods, energies, flows, games, diffusion, or transport.  \n",
        "This is one theory — expressed through many mathematical lenses.\n"
      ],
      "metadata": {
        "id": "vjUzZsDAKJUu"
      }
    }
  ]
}