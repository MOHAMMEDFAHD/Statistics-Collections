{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Families of Related Generative Modeling Techniques\n",
        "\n",
        "This section organizes major **generative modeling paradigms** by the *core difficulty they address* and the *mathematical idea they exploit*.  \n",
        "All of them attempt to solve the same fundamental problem:\n",
        "\n",
        "> **How can we transform a simple distribution into the data distribution using learnable stochastic or deterministic processes?**\n",
        "\n",
        "What differs is **how inference, sampling, and learning are approximated**.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Learning Generative Models via Approximate Inference\n",
        "\n",
        "### Core idea\n",
        "When **exact posterior inference is intractable**, learning must rely on approximations.\n",
        "\n",
        "### Wake–Sleep family\n",
        "Let the generative model be\n",
        "$$\n",
        "p_\\theta(x,z) = p_\\theta(z)\\,p_\\theta(x \\mid z)\n",
        "$$\n",
        "\n",
        "and an approximate inference model\n",
        "$$\n",
        "q_\\phi(z \\mid x)\n",
        "$$\n",
        "\n",
        "- **Wake phase**: update $\\theta$ using samples from $q_\\phi(z \\mid x)$  \n",
        "- **Sleep phase**: update $\\phi$ using samples from $p_\\theta(x,z)$\n",
        "\n",
        "### Reweighted Wake–Sleep (RWS)\n",
        "Uses **importance weighting** to reduce bias:\n",
        "$$\n",
        "w_i = \\frac{p_\\theta(x,z_i)}{q_\\phi(z_i \\mid x)}\n",
        "$$\n",
        "\n",
        "Learning signal approximates the true gradient of\n",
        "$$\n",
        "\\log p_\\theta(x)\n",
        "$$\n",
        "\n",
        "### Goal\n",
        "Train latent-variable models **without exact inference**, but with improved gradient estimates.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Markov Chain–Based Generative Models\n",
        "\n",
        "### Generative Stochastic Networks (GSNs)\n",
        "\n",
        "Instead of learning a static distribution, learn a **transition operator**:\n",
        "$$\n",
        "x_{t+1} \\sim p_\\theta(x_{t+1} \\mid x_t)\n",
        "$$\n",
        "\n",
        "The Markov chain is trained such that:\n",
        "$$\n",
        "\\lim_{t\\to\\infty} p(x_t) = p_{\\text{data}}(x)\n",
        "$$\n",
        "\n",
        "### Key properties\n",
        "- No explicit latent variables required\n",
        "- Sampling = **run the Markov chain**\n",
        "- The model learns **local transitions**, not a global density\n",
        "\n",
        "### Interpretation\n",
        "The generative model is defined implicitly by its **stationary distribution**.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Autoregressive Density Models\n",
        "\n",
        "### Fundamental factorization\n",
        "Any joint distribution can be written as:\n",
        "$$\n",
        "p(x_1,\\dots,x_d) = \\prod_{i=1}^d p(x_i \\mid x_{<i})\n",
        "$$\n",
        "\n",
        "### Neural Autoregressive Distribution Estimator (NADE)\n",
        "Each conditional is modeled with a neural network:\n",
        "$$\n",
        "p(x_i \\mid x_{<i}; \\theta)\n",
        "$$\n",
        "\n",
        "### Extensions\n",
        "- **Recurrent NADE**: sequences\n",
        "- **Deep NADE**: higher expressivity\n",
        "- **PixelCNN / WaveNet**: convolutional autoregressive structure\n",
        "\n",
        "### Properties\n",
        "- Exact likelihood\n",
        "- Exact sampling\n",
        "- No latent variables\n",
        "- Computational cost grows with dimensionality\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Adversarial Learning\n",
        "\n",
        "### Generative Adversarial Networks (GANs)\n",
        "\n",
        "Two-player game:\n",
        "- Generator: $x = G_\\theta(z)$\n",
        "- Discriminator: $D_\\phi(x) \\in [0,1]$\n",
        "\n",
        "Objective:\n",
        "$$\n",
        "\\min_G \\max_D \\;\n",
        "\\mathbb{E}_{x\\sim p_{\\text{data}}}[\\log D(x)]\n",
        "+ \\mathbb{E}_{z\\sim p(z)}[\\log(1-D(G(z)))]\n",
        "$$\n",
        "\n",
        "### Characteristics\n",
        "- No explicit likelihood\n",
        "- Training is **game-theoretic**, not probabilistic\n",
        "- Implicit density model\n",
        "\n",
        "### Conceptual ancestry\n",
        "Earlier ideas aimed at:\n",
        "- Learning inverse mappings\n",
        "- Factorized latent representations\n",
        "- Two-way encoder–decoder structures\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Invertible / Bijective Generative Models\n",
        "\n",
        "### Core structure\n",
        "A deterministic invertible map:\n",
        "$$\n",
        "x = f_\\theta(z), \\quad z = f_\\theta^{-1}(x)\n",
        "$$\n",
        "\n",
        "with simple base distribution:\n",
        "$$\n",
        "z \\sim p(z)\n",
        "$$\n",
        "\n",
        "### Exact likelihood (change of variables)\n",
        "$$\n",
        "\\log p(x) = \\log p(z) + \\log\\left|\\det \\frac{\\partial f^{-1}}{\\partial x}\\right|\n",
        "$$\n",
        "\n",
        "### Properties\n",
        "- Exact likelihood\n",
        "- Exact sampling\n",
        "- Latent variables are fully observed\n",
        "\n",
        "These ideas later unified under **normalizing flows**.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Learning Inverses of Bayesian Networks\n",
        "\n",
        "### Motivation\n",
        "Forward sampling in Bayesian networks:\n",
        "$$\n",
        "z \\rightarrow x\n",
        "$$\n",
        "is easy, but inference:\n",
        "$$\n",
        "x \\rightarrow z\n",
        "$$\n",
        "is hard.\n",
        "\n",
        "### Solution\n",
        "Learn a **stochastic inverse**:\n",
        "$$\n",
        "q_\\phi(z \\mid x)\n",
        "$$\n",
        "\n",
        "### Use cases\n",
        "- Approximate inference\n",
        "- Amortized inference\n",
        "- Efficient posterior sampling\n",
        "\n",
        "This idea foreshadows **variational inference** and encoder networks.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Conditional Gaussian Mixture Models\n",
        "\n",
        "### Mixtures of Conditional Gaussian Scale Mixtures (MCGSMs)\n",
        "\n",
        "Model local conditional densities:\n",
        "$$\n",
        "p(x_i \\mid \\text{context}) = \\sum_k \\pi_k(\\text{context})\\;\n",
        "\\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)\n",
        "$$\n",
        "\n",
        "### Characteristics\n",
        "- Parameters depend on local neighborhoods\n",
        "- Strong inductive bias for images\n",
        "- Pre-deep-learning state-of-the-art in vision\n",
        "\n",
        "### Interpretation\n",
        "Early structured probabilistic image models with learned local dependencies.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Early Neural Network Generative Models\n",
        "\n",
        "### Key ideas introduced\n",
        "- Neural networks as **generative mappings**\n",
        "- Latent manifolds\n",
        "- Stochastic decoding\n",
        "\n",
        "Form:\n",
        "$$\n",
        "x = f_\\theta(z) + \\epsilon\n",
        "$$\n",
        "\n",
        "### Contribution\n",
        "These works established:\n",
        "- Latent-variable thinking\n",
        "- Continuous data manifolds\n",
        "- Learned stochastic decoders\n",
        "\n",
        "They are direct conceptual ancestors of **VAEs** and **diffusion models**.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Physics-Inspired Sampling & Learning\n",
        "\n",
        "### Annealed Importance Sampling (AIS)\n",
        "\n",
        "Interpolate between distributions:\n",
        "$$\n",
        "p_0 \\rightarrow p_1 \\rightarrow \\dots \\rightarrow p_T\n",
        "$$\n",
        "\n",
        "Estimate ratios:\n",
        "$$\n",
        "\\frac{Z_T}{Z_0}\n",
        "$$\n",
        "\n",
        "Used for:\n",
        "- Partition function estimation\n",
        "- Energy-based models\n",
        "- Model comparison\n",
        "\n",
        "---\n",
        "\n",
        "### Langevin Dynamics\n",
        "\n",
        "Stochastic differential equation:\n",
        "$$\n",
        "dx_t = \\nabla_x \\log p(x_t)\\,dt + \\sqrt{2}\\,dW_t\n",
        "$$\n",
        "\n",
        "Discrete form:\n",
        "$$\n",
        "x_{t+1} = x_t + \\epsilon \\nabla_x \\log p(x_t) + \\sqrt{2\\epsilon}\\,\\xi_t\n",
        "$$\n",
        "\n",
        "Defines a diffusion whose stationary distribution is $p(x)$.\n",
        "\n",
        "---\n",
        "\n",
        "### Fokker–Planck Equation\n",
        "\n",
        "Evolution of density under diffusion:\n",
        "$$\n",
        "\\frac{\\partial p(x,t)}{\\partial t}\n",
        "=\n",
        "-\\nabla \\cdot (p \\nabla \\log p)\n",
        "+ \\Delta p\n",
        "$$\n",
        "\n",
        "Connects:\n",
        "- Stochastic dynamics\n",
        "- Density evolution\n",
        "- Thermodynamics\n",
        "\n",
        "---\n",
        "\n",
        "### Forward & Reverse Diffusion\n",
        "\n",
        "Forward process:\n",
        "$$\n",
        "dx = \\sqrt{\\beta(t)}\\,dW_t\n",
        "$$\n",
        "\n",
        "Reverse process exists:\n",
        "$$\n",
        "dx = \\big[\\nabla_x \\log p_t(x)\\big]\\,dt + \\sqrt{\\beta(t)}\\,d\\bar W_t\n",
        "$$\n",
        "\n",
        "This theoretical symmetry is the **foundation of diffusion generative models**.\n",
        "\n",
        "---\n",
        "\n",
        "## Big Picture Synthesis\n",
        "\n",
        "All these methods differ in *mechanism*, not in *purpose*.\n",
        "\n",
        "They answer the same question:\n",
        "\n",
        "> **How do we map a simple distribution into the data distribution?**\n",
        "\n",
        "Using:\n",
        "- Explicit likelihoods (autoregressive, flows)\n",
        "- Implicit distributions (GANs, GSNs)\n",
        "- Approximate inference (VAEs, wake–sleep)\n",
        "- Stochastic dynamics (diffusion, Langevin)\n",
        "\n",
        "Generative modeling is not a collection of tricks —  \n",
        "it is a single problem explored through multiple mathematical lenses.\n"
      ],
      "metadata": {
        "id": "BG9g5kAyJp8G"
      }
    }
  ]
}