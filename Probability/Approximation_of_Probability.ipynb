{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Approximation of Probability  \n",
        "How humans learned to reason when exact probability is impossible\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Why Approximation Is Inevitable\n",
        "\n",
        "Exact probability becomes impossible when:\n",
        "\n",
        "- integrals are high-dimensional,\n",
        "- normalization constants are unknown,\n",
        "- distributions have no closed form,\n",
        "- dependencies are dense or nonlinear.\n",
        "\n",
        "Formally, problems like:\n",
        "\n",
        "$$\n",
        "p(x) = \\int p(x, z)\\,dz\n",
        "$$\n",
        "\n",
        "$$\n",
        "Z = \\int e^{-E(x)}\\,dx\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbb{E}_p[f(x)]\n",
        "$$\n",
        "\n",
        "are provably intractable in general.\n",
        "\n",
        "So approximation is not a shortcut — it is the only option.\n",
        "\n",
        "Approximate probability is the science of controlled error.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. The Four Grand Approximation Strategies\n",
        "\n",
        "All approximation methods fall into four families:\n",
        "\n",
        "| Strategy | Replace probability with |\n",
        "|--------|--------------------------|\n",
        "| Optimization | A mode |\n",
        "| Local geometry | A Gaussian |\n",
        "| Sampling | Empirical averages |\n",
        "| Functional bounds | Simpler distributions |\n",
        "\n",
        "Everything else is a refinement.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Optimization-Based Approximation  \n",
        "Probability collapses into a single point\n",
        "\n",
        "### 3.1 Maximum Likelihood (MLE)\n",
        "\n",
        "$$\n",
        "\\theta^\\* = \\arg\\max_\\theta p(x \\mid \\theta)\n",
        "$$\n",
        "\n",
        "- Ignores uncertainty  \n",
        "- Keeps only the most plausible parameter  \n",
        "- Oldest approximation (Laplace, Gauss)\n",
        "\n",
        "---\n",
        "\n",
        "### 3.2 Maximum A Posteriori (MAP)\n",
        "\n",
        "$$\n",
        "\\theta^\\* = \\arg\\max_\\theta p(\\theta \\mid x)\n",
        "$$\n",
        "\n",
        "- Bayesian inference → optimization  \n",
        "- Replaces posterior distribution with a delta function  \n",
        "\n",
        "MAP approximates probability by belief in the best explanation.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Local Geometry Approximation  \n",
        "Probability is “almost Gaussian”\n",
        "\n",
        "### 4.1 Laplace Approximation\n",
        "\n",
        "Approximate posterior near mode:\n",
        "\n",
        "$$\n",
        "p(\\theta \\mid x) \\approx \\mathcal{N}(\\theta^\\*, H^{-1})\n",
        "$$\n",
        "\n",
        "Where  \n",
        "$$\n",
        "H\n",
        "$$\n",
        "is the Hessian of  \n",
        "$$\n",
        "-\\log p\n",
        "$$\n",
        "\n",
        "Assumes:\n",
        "\n",
        "- unimodality  \n",
        "- smooth curvature  \n",
        "\n",
        "Used in:\n",
        "\n",
        "- Bayesian statistics  \n",
        "- model evidence estimation  \n",
        "- early neural networks  \n",
        "\n",
        "---\n",
        "\n",
        "### 4.2 Saddle-Point & Asymptotic Expansions\n",
        "\n",
        "Use local curvature to approximate integrals:\n",
        "\n",
        "$$\n",
        "\\int e^{n f(x)}\\,dx \\approx e^{n f(x^\\*)}\n",
        "$$\n",
        "\n",
        "Extremely powerful for:\n",
        "\n",
        "- large-data regimes  \n",
        "- statistical physics  \n",
        "- information theory  \n",
        "\n",
        "---\n",
        "\n",
        "## 5. Sampling-Based Approximation  \n",
        "Probability becomes frequency\n",
        "\n",
        "### 5.1 Monte Carlo Approximation\n",
        "\n",
        "$$\n",
        "\\mathbb{E}_p[f(x)] \\approx \\frac{1}{N}\\sum_{i=1}^N f(x_i)\n",
        "$$\n",
        "\n",
        "Converts:\n",
        "\n",
        "- integrals → averages  \n",
        "- uncertainty → randomness  \n",
        "\n",
        "Key theorem: Law of Large Numbers\n",
        "\n",
        "---\n",
        "\n",
        "### 5.2 Importance Sampling\n",
        "\n",
        "Rewrite:\n",
        "\n",
        "$$\n",
        "\\mathbb{E}_p[f(x)]\n",
        "=\n",
        "\\mathbb{E}_q\\!\\left[f(x)\\frac{p(x)}{q(x)}\\right]\n",
        "$$\n",
        "\n",
        "Trade:\n",
        "\n",
        "- difficult distribution  \n",
        "- for easier one + weights  \n",
        "\n",
        "Failure mode: weight degeneracy\n",
        "\n",
        "---\n",
        "\n",
        "### 5.3 Markov Chain Monte Carlo (MCMC)\n",
        "\n",
        "Avoid normalization entirely.\n",
        "\n",
        "Construct a chain with:\n",
        "\n",
        "$$\n",
        "\\pi(x) = p(x)\n",
        "$$\n",
        "\n",
        "Methods:\n",
        "\n",
        "- Metropolis–Hastings  \n",
        "- Gibbs sampling  \n",
        "- Hamiltonian Monte Carlo  \n",
        "- Langevin dynamics  \n",
        "\n",
        "MCMC approximates probability by time.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Variational Approximation  \n",
        "Probability becomes a solvable optimization problem\n",
        "\n",
        "### 6.1 Variational Inference (VI)\n",
        "\n",
        "Replace intractable  \n",
        "$$\n",
        "p\n",
        "$$\n",
        "with tractable  \n",
        "$$\n",
        "q\n",
        "$$\n",
        "\n",
        "$$\n",
        "q^\\* = \\arg\\min_{q \\in \\mathcal{Q}} \\mathrm{KL}(q \\| p)\n",
        "$$\n",
        "\n",
        "Equivalent to maximizing:\n",
        "\n",
        "$$\n",
        "\\mathrm{ELBO}\n",
        "=\n",
        "\\mathbb{E}_q[\\log p(x,z)]\n",
        "-\n",
        "\\mathbb{E}_q[\\log q(z)]\n",
        "$$\n",
        "\n",
        "Key idea:\n",
        "\n",
        "Approximate inference becomes function fitting.\n",
        "\n",
        "---\n",
        "\n",
        "### 6.2 Mean-Field Approximation\n",
        "\n",
        "Assume:\n",
        "\n",
        "$$\n",
        "q(z) = \\prod_i q_i(z_i)\n",
        "$$\n",
        "\n",
        "- Breaks dependencies  \n",
        "- Makes inference tractable  \n",
        "- Introduces bias  \n",
        "\n",
        "Used heavily in:\n",
        "\n",
        "- physics  \n",
        "- large Bayesian models  \n",
        "- VAEs  \n",
        "\n",
        "---\n",
        "\n",
        "### 6.3 Expectation Propagation (EP)\n",
        "\n",
        "- Instead of global KL: match local moments  \n",
        "- Refine approximations iteratively  \n",
        "- Often more accurate but less stable  \n",
        "\n",
        "---\n",
        "\n",
        "## 7. Discretization & Relaxation  \n",
        "Turn continuous into computable\n",
        "\n",
        "### 7.1 Grid-Based Approximation\n",
        "\n",
        "- Discretize space  \n",
        "- Approximate integrals with sums  \n",
        "- Curse of dimensionality applies  \n",
        "\n",
        "---\n",
        "\n",
        "### 7.2 Relaxation of Constraints\n",
        "\n",
        "- Replace hard constraints with soft penalties  \n",
        "- Common in optimization-based probabilistic models  \n",
        "\n",
        "---\n",
        "\n",
        "## 8. Asymptotic Probability  \n",
        "Let infinity do the work\n",
        "\n",
        "### 8.1 Law of Large Numbers\n",
        "\n",
        "Random → deterministic\n",
        "\n",
        "### 8.2 Central Limit Theorem\n",
        "\n",
        "Everything → Gaussian\n",
        "\n",
        "### 8.3 Large Deviations Theory\n",
        "\n",
        "Rare events become exponentially structured\n",
        "\n",
        "This justifies:\n",
        "\n",
        "- Gaussian approximations  \n",
        "- confidence intervals  \n",
        "- error bars  \n",
        "\n",
        "---\n",
        "\n",
        "## 9. Functional Approximation  \n",
        "Approximate probability indirectly\n",
        "\n",
        "### 9.1 Energy-Based Approximation\n",
        "\n",
        "Model:\n",
        "\n",
        "$$\n",
        "p(x) = \\frac{e^{-E(x)}}{Z}\n",
        "$$\n",
        "\n",
        "Approximate:\n",
        "\n",
        "- gradients  \n",
        "- energy differences  \n",
        "\n",
        "not  \n",
        "$$\n",
        "Z\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 9.2 Score Approximation\n",
        "\n",
        "Learn:\n",
        "\n",
        "$$\n",
        "\\nabla_x \\log p(x)\n",
        "$$\n",
        "\n",
        "Instead of  \n",
        "$$\n",
        "p(x)\n",
        "$$\n",
        "\n",
        "Used in:\n",
        "\n",
        "- score matching  \n",
        "- diffusion models  \n",
        "\n",
        "---\n",
        "\n",
        "## 10. Dynamic Approximation  \n",
        "Replace static probability with evolution\n",
        "\n",
        "### 10.1 Langevin Dynamics\n",
        "\n",
        "$$\n",
        "x_{t+1}\n",
        "=\n",
        "x_t\n",
        "+\n",
        "\\epsilon \\nabla \\log p(x_t)\n",
        "+\n",
        "\\sqrt{2\\epsilon}\\,\\xi_t\n",
        "$$\n",
        "\n",
        "Approximates sampling via stochastic motion.\n",
        "\n",
        "---\n",
        "\n",
        "### 10.2 Diffusion Processes\n",
        "\n",
        "- Gradually destroy structure  \n",
        "- Then reverse it  \n",
        "\n",
        "This approximates:\n",
        "\n",
        "- complex distributions  \n",
        "- without ever evaluating likelihoods  \n",
        "\n",
        "---\n",
        "\n",
        "## 11. Learning-Based Approximation  \n",
        "Let neural networks approximate probability\n",
        "\n",
        "### 11.1 Autoregressive Models\n",
        "\n",
        "Exact factorization, approximate conditionals\n",
        "\n",
        "### 11.2 Normalizing Flows\n",
        "\n",
        "Exact likelihood, approximate transformations\n",
        "\n",
        "### 11.3 Implicit Models (GANs)\n",
        "\n",
        "Approximate samples, no density\n",
        "\n",
        "---\n",
        "\n",
        "## 12. Approximation Tradeoffs (The Triangle)\n",
        "\n",
        "Every approximation chooses two:\n",
        "\n",
        "| Property | Cost |\n",
        "|--------|------|\n",
        "| Accuracy | Computation |\n",
        "| Tractability | Bias |\n",
        "| Speed | Variance |\n",
        "\n",
        "No free lunch.\n",
        "\n",
        "---\n",
        "\n",
        "## 13. Deep Unifying Insight\n",
        "\n",
        "Exact probability is a mathematical object.  \n",
        "Approximate probability is an epistemic stance.\n",
        "\n",
        "Each approximation answers:\n",
        "\n",
        "- What do I care about preserving?  \n",
        "- mass?  \n",
        "- modes?  \n",
        "- moments?  \n",
        "- samples?  \n",
        "\n",
        "Different methods preserve different truths.\n",
        "\n",
        "---\n",
        "\n",
        "## 14. Final Synthesis\n",
        "\n",
        "Approximation turns probability from an object you cannot compute into a belief you can act upon.\n",
        "\n",
        "It does so by:\n",
        "\n",
        "- collapsing distributions,  \n",
        "- smoothing geometry,  \n",
        "- averaging randomness,  \n",
        "- or bounding uncertainty.\n"
      ],
      "metadata": {
        "id": "yfeyvxSU0_hk"
      }
    }
  ]
}