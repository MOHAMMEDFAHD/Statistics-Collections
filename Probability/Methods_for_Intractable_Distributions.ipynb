{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "I. Analytical & Structural Simplifications\n",
        "\n",
        "(Reduce complexity by exploiting structure)\n",
        "\n",
        "1. Factorization & Conditional Independence\n",
        "\n",
        "Bayesian networks  \n",
        "Markov random fields  \n",
        "Graphical model decomposition  \n",
        "Chain rule factorization  \n",
        "\n",
        "Key idea:\n",
        "\n",
        "$$\n",
        "p(x_1, \\ldots, x_n) = \\prod_i p\\left(x_i \\mid \\text{parents}(x_i)\\right)\n",
        "$$\n",
        "\n",
        "2. Sufficient Statistics\n",
        "\n",
        "Exponential family models  \n",
        "Dimension reduction without loss of information  \n",
        "\n",
        "$$\n",
        "p(x \\mid \\theta) = h(x)\\exp\\left(\\eta(\\theta)^{\\top} T(x) - A(\\theta)\\right)\n",
        "$$\n",
        "\n",
        "3. Conjugate Priors\n",
        "\n",
        "Closed-form posteriors  \n",
        "Avoid numerical integration  \n",
        "\n",
        "Examples:  \n",
        "Beta–Binomial  \n",
        "Dirichlet–Multinomial  \n",
        "Gaussian–Gaussian  \n",
        "\n",
        "4. Symmetry & Invariance Exploitation\n",
        "\n",
        "Exchangeability  \n",
        "Stationarity  \n",
        "Rotational / translational invariance  \n",
        "\n",
        "Example: de Finetti’s theorem  \n",
        "\n",
        "---\n",
        "\n",
        "II. Approximation of Distributions\n",
        "\n",
        "(Replace the true distribution with a tractable surrogate)\n",
        "\n",
        "5. Moment-Based Approximations\n",
        "\n",
        "Mean-field approximations  \n",
        "Moment closure methods  \n",
        "Gaussian approximations via mean and covariance  \n",
        "\n",
        "6. Variational Inference (VI)\n",
        "\n",
        "Replace intractable posterior  \n",
        "\n",
        "$$\n",
        "p(z \\mid x)\n",
        "$$\n",
        "\n",
        "with tractable  \n",
        "\n",
        "$$\n",
        "q(z)\n",
        "$$\n",
        "\n",
        "Optimization objective:\n",
        "\n",
        "$$\n",
        "\\min_{q \\in \\mathcal{Q}} \\mathrm{KL}\\big(q(z) \\,\\|\\, p(z \\mid x)\\big)\n",
        "$$\n",
        "\n",
        "Variants:  \n",
        "Mean-field VI  \n",
        "Structured VI  \n",
        "Black-box VI  \n",
        "Stochastic VI  \n",
        "Amortized VI (VAEs)  \n",
        "\n",
        "7. Laplace Approximation\n",
        "\n",
        "Second-order Taylor expansion around MAP:\n",
        "\n",
        "$$\n",
        "p(\\theta \\mid x) \\approx \\mathcal{N}\\left(\\hat{\\theta}, H^{-1}\\right)\n",
        "$$\n",
        "\n",
        "8. Expectation Propagation (EP)\n",
        "\n",
        "Local moment matching  \n",
        "Replaces factors with Gaussian approximations  \n",
        "\n",
        "9. Saddle-Point & Asymptotic Approximations\n",
        "\n",
        "Large-sample limits  \n",
        "Stationary phase methods  \n",
        "WKB approximations  \n",
        "\n",
        "---\n",
        "\n",
        "III. Sampling-Based Methods\n",
        "\n",
        "(Avoid computing distributions explicitly)\n",
        "\n",
        "10. Monte Carlo Integration\n",
        "\n",
        "Estimate expectations directly:\n",
        "\n",
        "$$\n",
        "\\mathbb{E}[f(X)] \\approx \\frac{1}{N}\\sum_{i=1}^{N} f(x_i)\n",
        "$$\n",
        "\n",
        "11. Markov Chain Monte Carlo (MCMC)\n",
        "\n",
        "Circumvent normalization constants entirely.\n",
        "\n",
        "Key families:  \n",
        "Metropolis–Hastings  \n",
        "Gibbs sampling  \n",
        "Hamiltonian Monte Carlo (HMC)  \n",
        "Langevin dynamics  \n",
        "Slice sampling  \n",
        "\n",
        "12. Sequential Monte Carlo (SMC)\n",
        "\n",
        "Particle filters  \n",
        "Annealed importance sampling  \n",
        "Tempered transitions  \n",
        "\n",
        "13. Importance Sampling\n",
        "\n",
        "Reweight samples from easier proposal $q(x)$:\n",
        "\n",
        "$$\n",
        "\\mathbb{E}_p[f] = \\mathbb{E}_q\\left[f(x)\\frac{p(x)}{q(x)}\\right]\n",
        "$$\n",
        "\n",
        "14. Pseudo-Marginal Methods\n",
        "\n",
        "Use unbiased estimators of likelihoods  \n",
        "Enables MCMC without exact likelihoods  \n",
        "\n",
        "---\n",
        "\n",
        "IV. Transformational Techniques\n",
        "\n",
        "(Change the problem representation)\n",
        "\n",
        "15. Change of Variables & Normalizing Flows\n",
        "\n",
        "Invertible transformations:\n",
        "\n",
        "$$\n",
        "z = f(x), \\quad p(x) = p(z)\\left|\\det J_f(x)\\right|\n",
        "$$\n",
        "\n",
        "Examples:  \n",
        "RealNVP  \n",
        "Glow  \n",
        "Neural spline flows  \n",
        "\n",
        "16. Data Augmentation\n",
        "\n",
        "Introduce latent variables to simplify conditionals.\n",
        "\n",
        "Examples:  \n",
        "EM algorithm  \n",
        "Polya–Gamma augmentation  \n",
        "Auxiliary variable methods  \n",
        "\n",
        "17. Latent Variable Models\n",
        "\n",
        "Mixture models  \n",
        "Hidden Markov Models  \n",
        "State-space models  \n",
        "\n",
        "---\n",
        "\n",
        "V. Optimization-Driven Surrogates\n",
        "\n",
        "(Convert probabilistic inference into optimization)\n",
        "\n",
        "18. MAP Estimation\n",
        "\n",
        "Ignore normalization:\n",
        "\n",
        "$$\n",
        "\\arg\\max_{\\theta} p(\\theta \\mid x)\n",
        "$$\n",
        "\n",
        "19. Penalized Likelihood & Regularization\n",
        "\n",
        "Lasso  \n",
        "Ridge  \n",
        "Elastic net  \n",
        "\n",
        "Bayesian interpretation: implicit priors  \n",
        "\n",
        "20. Score Matching & Contrastive Divergence\n",
        "\n",
        "Avoid partition function computation.\n",
        "\n",
        "Used in:  \n",
        "Energy-based models  \n",
        "Restricted Boltzmann Machines  \n",
        "\n",
        "---\n",
        "\n",
        "VI. Information-Theoretic Methods\n",
        "\n",
        "(Replace probability with divergence minimization)\n",
        "\n",
        "21. Entropy Maximization\n",
        "\n",
        "MaxEnt principle  \n",
        "Leads to exponential family forms  \n",
        "\n",
        "22. f-Divergence Minimization\n",
        "\n",
        "KL  \n",
        "Jensen–Shannon  \n",
        "Wasserstein distance  \n",
        "\n",
        "Used in:  \n",
        "GANs  \n",
        "Variational objectives  \n",
        "\n",
        "---\n",
        "\n",
        "VII. Stochastic Differential & Continuous Limits\n",
        "\n",
        "(Model distributions via dynamics)\n",
        "\n",
        "23. Stochastic Differential Equations (SDEs)\n",
        "\n",
        "Langevin diffusion  \n",
        "Fokker–Planck equations  \n",
        "\n",
        "Stationary distribution:\n",
        "\n",
        "$$\n",
        "dX_t = \\nabla \\log p(X_t)\\,dt + \\sqrt{2}\\,dW_t\n",
        "$$\n",
        "\n",
        "24. Diffusion Models\n",
        "\n",
        "Score-based generative models  \n",
        "Reverse-time SDEs  \n",
        "\n",
        "---\n",
        "\n",
        "VIII. Discretization & Numerical Schemes\n",
        "\n",
        "(Approximate continuous objects numerically)\n",
        "\n",
        "25. Quadrature & Numerical Integration\n",
        "\n",
        "Gaussian quadrature  \n",
        "Sparse grids  \n",
        "Monte Carlo quadrature  \n",
        "\n",
        "26. Grid-Based & Histogram Methods\n",
        "\n",
        "Curse of dimensionality limits applicability  \n",
        "\n",
        "---\n",
        "\n",
        "IX. Model Reduction & Coarse-Graining\n",
        "\n",
        "(Reduce state space)\n",
        "\n",
        "27. State Aggregation\n",
        "\n",
        "Lumpability in Markov chains  \n",
        "Reduced order models  \n",
        "\n",
        "28. Projection Methods\n",
        "\n",
        "Principal component projections  \n",
        "Reduced sufficient statistics  \n",
        "\n",
        "---\n",
        "\n",
        "X. Hybrid & Modern AI-Driven Approaches\n",
        "\n",
        "(Learn the distribution implicitly)\n",
        "\n",
        "29. Implicit Generative Models\n",
        "\n",
        "GANs  \n",
        "Energy-based neural models  \n",
        "\n",
        "No explicit likelihood needed.  \n",
        "\n",
        "30. Amortized Inference\n",
        "\n",
        "Neural networks learn inference maps  \n",
        "Used in VAEs and simulators  \n",
        "\n",
        "31. Likelihood-Free Inference (ABC)\n",
        "\n",
        "Approximate Bayesian Computation:\n",
        "\n",
        "$$\n",
        "p\\big(\\theta \\mid \\rho(S(x), S(x^\\ast)) < \\varepsilon\\big)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "XI. Philosophical Meta-Strategies\n",
        "\n",
        "(How experts think about intractability)\n",
        "\n",
        "32. Replace Exactness with Guarantees\n",
        "\n",
        "Bounds instead of values  \n",
        "Concentration inequalities  \n",
        "PAC-Bayesian bounds  \n",
        "\n",
        "33. Replace Distribution with Expectations\n",
        "\n",
        "Predictive distributions replaced by moments  \n",
        "Risk minimization frameworks  \n",
        "\n",
        "---\n",
        "\n",
        "XII. Summary Table (Mental Model)\n",
        "\n",
        "| Strategy             | Core Idea                         |\n",
        "|----------------------|-----------------------------------|\n",
        "| Analytical           | Exploit structure                 |\n",
        "| Approximation        | Replace distribution              |\n",
        "| Sampling             | Avoid density evaluation           |\n",
        "| Transformation       | Change variables                  |\n",
        "| Optimization         | Ignore normalization              |\n",
        "| Information theory   | Minimize divergence               |\n",
        "| Dynamics             | Simulate stationary laws           |\n",
        "| Reduction            | Shrink state space                |\n",
        "| Learning             | Learn distribution implicitly     |\n",
        "\n",
        "Final Expert Insight\n",
        "\n",
        "Intractability is not a failure; it is the signal that probability must be approached indirectly.  \n",
        "Every successful probabilistic method ever devised sidesteps the distribution rather than confronting it head-on.\n"
      ],
      "metadata": {
        "id": "lMex_PmmxuPg"
      }
    }
  ]
}