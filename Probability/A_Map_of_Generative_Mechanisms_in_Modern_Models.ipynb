{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A Map of Generative Mechanisms in Modern Models\n",
        "\n",
        "Every modern generative model relies on one fundamental mechanism — or a combination of several — to produce data.  \n",
        "These mechanisms define **how generation happens**, independent of architecture or training details.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Markov Chain Sampling\n",
        "\n",
        "**Heritage:** Boltzmann Machines (Hinton)\n",
        "\n",
        "### Mechanism\n",
        "$$\n",
        "x_{t+1} \\sim p(x \\mid x_t)\n",
        "$$\n",
        "\n",
        "The next state depends only on the current state.  \n",
        "Generation is performed by running the chain until it reaches equilibrium.  \n",
        "There is no direct or instantaneous output; samples emerge only after sufficient mixing.\n",
        "\n",
        "### Examples\n",
        "- Boltzmann Machine  \n",
        "- Restricted Boltzmann Machine  \n",
        "- Deep Belief Networks  \n",
        "\n",
        "In this setting, generation is equivalent to a **physical simulation** driven by temperature, energy, and equilibrium.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Autoregressive Factorization\n",
        "\n",
        "**Heritage:** Language Modeling → GPT\n",
        "\n",
        "### Mechanism\n",
        "$$\n",
        "p(x)\n",
        "=\n",
        "\\prod_t p(x_t \\mid x_{<t})\n",
        "$$\n",
        "\n",
        "Only one element is generated at each step.  \n",
        "This is **not Markovian** in the classical sense, because the entire previous context is retained as memory rather than only the last state.\n",
        "\n",
        "### Examples\n",
        "- GPT  \n",
        "- PixelRNN / PixelCNN  \n",
        "- WaveNet  \n",
        "\n",
        "Generation here is **repeated local prediction**, not physical simulation.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Latent Variable Decoding (Deterministic Mapping)\n",
        "\n",
        "**Heritage:** Variational Autoencoders (Kingma & Welling)\n",
        "\n",
        "### Mechanism\n",
        "$$\n",
        "z \\sim p(z), \\qquad x = f_\\theta(z)\n",
        "$$\n",
        "\n",
        "Generation occurs in a single step.  \n",
        "Randomness exists only in the latent space.  \n",
        "There is no chain and no temporal dynamics.\n",
        "\n",
        "### Examples\n",
        "- VAE  \n",
        "- Conditional VAE  \n",
        "- β-VAE  \n",
        "\n",
        "Generation is best understood as a **geometric projection** from a low-dimensional space to a high-dimensional one.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Adversarial Game (Implicit Distribution Learning)\n",
        "\n",
        "**Heritage:** GANs (Goodfellow)\n",
        "\n",
        "### Mechanism\n",
        "$$\n",
        "z \\sim p(z), \\qquad x = G(z)\n",
        "$$\n",
        "\n",
        "There is no explicit likelihood, no analytically defined generative path, and no tractable probability density.  \n",
        "The distribution is learned implicitly through competition.\n",
        "\n",
        "### Examples\n",
        "- GAN  \n",
        "- StyleGAN  \n",
        "- BigGAN  \n",
        "\n",
        "Generation is a consequence of **training dynamics**, not a formally specified probabilistic mechanism.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Invertible Flow-Based Transformation\n",
        "\n",
        "**Heritage:** Flow-Based Models\n",
        "\n",
        "### Mechanism\n",
        "$$\n",
        "x = f(z), \\qquad z = f^{-1}(x)\n",
        "$$\n",
        "\n",
        "The model consists of invertible transformations with a tractable Jacobian.  \n",
        "Sampling is direct and exact.\n",
        "\n",
        "### Examples\n",
        "- NICE  \n",
        "- RealNVP  \n",
        "- Glow  \n",
        "\n",
        "Generation is simply the **inverse of a mathematical function** — no simulation and no adversarial process are involved.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Stochastic Differential Equation Dynamics\n",
        "\n",
        "**Heritage:** Diffusion and Score-Based Models\n",
        "\n",
        "### Mechanism\n",
        "$$\n",
        "dx = f(x,t)\\,dt + g(t)\\,dW\n",
        "$$\n",
        "\n",
        "Noise is progressively added and then reversed.  \n",
        "Generation corresponds to solving a stochastic differential equation, where time plays a central role.\n",
        "\n",
        "### Examples\n",
        "- DDPM  \n",
        "- Score-based SDE models  \n",
        "- EDM  \n",
        "\n",
        "Generation here is the **simulation of a temporal trajectory**, closer to physics than classical statistics.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Score Field Following\n",
        "\n",
        "**Heritage:** Yang Song\n",
        "\n",
        "### Mechanism\n",
        "$$\n",
        "x_{t+1}\n",
        "=\n",
        "x_t\n",
        "+\n",
        "\\epsilon \\nabla_x \\log p(x_t)\n",
        "+\n",
        "\\sqrt{2\\epsilon}\\,\\xi\n",
        "$$\n",
        "\n",
        "There is no likelihood and no explicit probability distribution.  \n",
        "The model learns only the **direction of movement** in data space.\n",
        "\n",
        "### Examples\n",
        "- Score Matching Models  \n",
        "- Langevin Dynamics  \n",
        "\n",
        "Generation corresponds to **following a learned vector field**, rather than computing probabilities.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Hybrid Mechanisms\n",
        "\n",
        "**Modern Developments**\n",
        "\n",
        "Many recent models combine multiple mechanisms:\n",
        "\n",
        "- Diffusion Transformers: SDEs + Attention  \n",
        "- Autoregressive Diffusion: Autoregression + Noise  \n",
        "- Energy-Based Diffusion: Energy Models + Score Fields  \n",
        "\n",
        "The future of generative modeling is **not dominated by a single mechanism**.\n",
        "\n",
        "---\n",
        "\n",
        "## Philosophical Summary\n",
        "\n",
        "All modern generative models share a common principle:\n",
        "\n",
        "They do not attempt to model the full data distribution directly.\n",
        "\n",
        "Instead, they select a **generative mechanism** that can be executed stably and efficiently.\n",
        "\n",
        "These mechanisms can be reduced to a small set of fundamental ideas:\n",
        "\n",
        "- **Simulation** (Markov chains, SDEs)  \n",
        "- **Factorization** (autoregressive models)  \n",
        "- **Transformation** (latent mappings, flows)  \n",
        "- **Games** (adversarial learning)  \n",
        "- **Direction following** (score-based dynamics)\n",
        "\n",
        "Generative modeling progresses not by conquering the full probability space,  \n",
        "but by choosing the **right dynamics through which data can emerge**.\n"
      ],
      "metadata": {
        "id": "RWpfUyf4Kijs"
      }
    }
  ]
}